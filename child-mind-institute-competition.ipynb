{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":81933,"databundleVersionId":9643020,"sourceType":"competition"}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**SEMI SUPERVISED** without actigraphy","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\ntrain_df=pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/train.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.models import Model\nfrom keras.layers import Input, Dense, Dropout\n\n# Mapping for converting season names to numerical values\nseason_mapping = {\n    'Fall': 0,\n    'Winter': 1,\n    'Spring': 2,\n    'Summer': 3,\n}\n\n# Initialize an empty list to store column names that contain 'Season'\nseason_columns = []\n\n# Iterate over the columns of the training DataFrame to find columns related to seasons\nfor col in train_df.keys():\n    if 'Season' in col:  # Check if 'Season' is part of the column name\n        season_columns.append(col)  # Add the column to the season_columns list\n\n# Print the columns identified as season-related for verification\nprint(season_columns)\n\n# Replace the season names in the identified columns with their corresponding numerical values\ntrain_df[season_columns] = train_df[season_columns].replace(season_mapping)\n\n# Display the updated DataFrame\ntrain_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.model_selection import train_test_split\n\n# Split the DataFrame into features (X) and target (y)\nX = train_df.drop(columns=['sii'])  # Drop the 'sii' column to use the rest as features\ny = train_df['sii']  # The target variable is 'sii'\n\n# Split the data into training+testing and inference sets\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.1, random_state=42  # Use 10% of the data for testing; set a random state for reproducibility\n)\n\n# Optional second split for validation (commented out here):\n# Split the training+testing set further into training and validation sets\n# X_train, X_val, y_train, y_val = train_test_split(\n#     X_train_val, y_train_val, test_size=0.2, random_state=42\n# )  \n# This would create a validation set that is 20% of the training set, equivalent to 0.2 * 0.9 = 18% of the total data\n\n# Print the unique values in the training target variable for verification\nprint(np.unique(y_train))\n\n# Display the feature set (optional, for checking)\nX","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nfrom tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization, Concatenate\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\nfrom tensorflow.keras.regularizers import l2\nfrom tensorflow.keras.utils import to_categorical\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\nimport tensorflow as tf\n\ndef create_dual_input_model(base_input_dim, extra_features_dim, encoding_dim=32, dropout_rate=0.3):\n    \"\"\"\n    Creates a model with two training modes:\n    1. Unsupervised learning with only base features (autoencoder).\n    2. Supervised learning with combined base and extra features.\n    \"\"\"\n    # Input layer for base features\n    base_input = Input(shape=(base_input_dim,), name='base_input')\n\n    # Encoder layers for base features\n    encoded = Dense(64, activation='relu', kernel_regularizer=l2(1e-4))(base_input)\n    encoded = BatchNormalization()(encoded)\n    encoded = Dropout(dropout_rate)(encoded)\n\n    encoded = Dense(48, activation='relu', kernel_regularizer=l2(1e-4))(encoded)\n    encoded = BatchNormalization()(encoded)\n    encoded = Dropout(dropout_rate)(encoded)\n\n    # Bottleneck layer for encoding base features\n    bottleneck = Dense(encoding_dim, activation='relu', name='bottleneck')(encoded)\n\n    # Create encoder model (for feature extraction)\n    encoder = Model(inputs=base_input, outputs=bottleneck, name='encoder')\n\n    # Decoder layers for reconstructing input (autoencoder)\n    decoded = Dense(48, activation='relu', kernel_regularizer=l2(1e-4))(bottleneck)\n    decoded = BatchNormalization()(decoded)\n    decoded = Dropout(dropout_rate)(decoded)\n\n    decoded = Dense(64, activation='relu', kernel_regularizer=l2(1e-4))(decoded)\n    decoded = BatchNormalization()(decoded)\n    decoded = Dropout(dropout_rate)(decoded)\n\n    # Output layer for autoencoder\n    decoder_output = Dense(base_input_dim, activation='sigmoid')(decoded)\n\n    # Create autoencoder model (base_input -> reconstruction)\n    autoencoder = Model(inputs=base_input, outputs=decoder_output, name='autoencoder')\n\n    # Input layer for extra features (only used in supervised training)\n    extra_input = Input(shape=(extra_features_dim,), name='extra_input')\n\n    # Concatenate bottleneck output with extra features for supervised training\n    combined = Concatenate()([bottleneck, extra_input])\n\n    # Fully connected layers for supervised learning\n    supervised = Dense(96, activation='relu', kernel_regularizer=l2(1e-4))(combined)\n    supervised = BatchNormalization()(supervised)\n    supervised = Dropout(dropout_rate)(supervised)\n\n    supervised = Dense(64, activation='relu', kernel_regularizer=l2(1e-4))(supervised)\n    supervised = BatchNormalization()(supervised)\n    supervised = Dropout(dropout_rate)(supervised)\n\n    supervised = Dense(32, activation='relu', kernel_regularizer=l2(1e-4))(supervised)\n    supervised = BatchNormalization()(supervised)\n    supervised = Dropout(dropout_rate)(supervised)\n\n    # Output layer for classification (adjust number of classes as needed)\n    output = Dense(4, activation='softmax')(supervised)\n\n    # Create supervised model (combined inputs -> classification output)\n    supervised_model = Model(inputs=[bottleneck, extra_input], outputs=output, name='supervised')\n\n    return autoencoder, encoder, supervised_model\n\ndef prepare_data_for_training(X_unlabeled, X_labeled, extra_features_cols, impute_strategy='median', normalize=True):\n    \"\"\"\n    Prepares data for training by imputing missing values and normalizing.\n    \"\"\"\n    # Identify base features (columns not in extra_features_cols)\n    base_features = [col for col in X_unlabeled.columns if col not in extra_features_cols]\n    print(base_features)\n\n    # Impute missing values using the specified strategy\n    imputer = SimpleImputer(strategy=impute_strategy)\n\n    # Impute and transform base features for unlabeled data\n    X_unlabeled_base = X_unlabeled[base_features].copy()\n    X_unlabeled_base_imputed = imputer.fit_transform(X_unlabeled_base)\n\n    # Impute and transform base features for labeled data\n    X_labeled_base = X_labeled[base_features].copy()\n    X_labeled_base_imputed = imputer.fit_transform(X_labeled_base)\n\n    # Impute and transform extra features for labeled data\n    X_labeled_extra = X_labeled[extra_features_cols].copy()\n    X_labeled_extra_imputed = imputer.fit_transform(X_labeled_extra)\n\n    # Normalize data if specified\n    if normalize:\n        scaler = MinMaxScaler()  # StandardScaler() can be used as an alternative\n        X_unlabeled_base_imputed = scaler.fit_transform(X_unlabeled_base_imputed)\n        X_labeled_base_imputed = scaler.fit_transform(X_labeled_base_imputed)\n        X_labeled_extra_imputed = scaler.fit_transform(X_labeled_extra_imputed)\n\n    # Return processed data as DataFrames for consistency\n    return (\n        pd.DataFrame(X_unlabeled_base_imputed, columns=base_features),\n        pd.DataFrame(X_labeled_base_imputed, columns=base_features),\n        pd.DataFrame(X_labeled_extra_imputed, columns=extra_features_cols)\n    )\n\ndef train_models(X_unlabeled, X_labeled, y_labeled, extra_features_cols, encoding_dim=32):\n    \"\"\"\n    Trains both unsupervised (autoencoder) and supervised models.\n    \"\"\"\n    # Prepare data for training\n    X_unlabeled_base, X_labeled_base, X_labeled_extra = prepare_data_for_training(\n        X_unlabeled, X_labeled, extra_features_cols\n    )\n\n    print('X labeled base', X_unlabeled_base.shape, 'labeled data', X_labeled_base.shape, 'labeled data extra features', X_labeled_extra.shape)\n\n    # Create models\n    autoencoder, encoder, supervised_model = create_dual_input_model(\n        base_input_dim=len(X_unlabeled_base.columns),  # Input dimension for base features\n        extra_features_dim=len(extra_features_cols),   # Input dimension for extra features\n        encoding_dim=encoding_dim                      # Size of encoding layer\n    )\n\n    # Compile the models with appropriate loss and optimization functions\n    autoencoder.compile(optimizer='adam', loss='mean_squared_error')\n    supervised_model.compile(optimizer='adam', \n                             loss='categorical_crossentropy',\n                             metrics=['accuracy'])\n\n    # Train the autoencoder\n    autoencoder_history = autoencoder.fit(\n        X_unlabeled_base, X_unlabeled_base,  # Input and target are the same for reconstruction\n        epochs=100,\n        batch_size=32,\n        validation_split=0.1,\n        callbacks=[\n            EarlyStopping(patience=10, restore_best_weights=True),\n            ReduceLROnPlateau(factor=0.2, patience=5)\n        ]\n    )\n\n    # Use the encoder to extract features from labeled data\n    encoded_features = encoder.predict(X_labeled_base)\n\n    # Check shapes to ensure compatibility for training the supervised model\n    print(f'Encoded features shape: {encoded_features.shape}')\n    print(f'Extra features shape: {X_labeled_extra.shape}')\n    assert encoded_features.shape[1] == encoding_dim, \"Mismatch in encoded features shape\"\n    assert X_labeled_extra.shape[1] == len(extra_features_cols), \"Mismatch in extra features shape\"\n\n    # Train the supervised model\n    supervised_history = supervised_model.fit(\n        [encoded_features, X_labeled_extra],  # Combined inputs\n        y_labeled,\n        epochs=100,\n        batch_size=32,\n        validation_split=0.1,\n        callbacks=[\n            EarlyStopping(patience=10, restore_best_weights=True),\n            ReduceLROnPlateau(factor=0.2, patience=5)\n        ]\n    )\n\n    return autoencoder, encoder, supervised_model, autoencoder_history, supervised_history\n\n# Main code for preparing data and training models\ncolumns = ['PCIAT-Season', 'PCIAT-PCIAT_01', 'PCIAT-PCIAT_02', 'PCIAT-PCIAT_03', \n           'PCIAT-PCIAT_04', 'PCIAT-PCIAT_05', 'PCIAT-PCIAT_06', 'PCIAT-PCIAT_07', \n           'PCIAT-PCIAT_08', 'PCIAT-PCIAT_09', 'PCIAT-PCIAT_10', 'PCIAT-PCIAT_11', \n           'PCIAT-PCIAT_12', 'PCIAT-PCIAT_13', 'PCIAT-PCIAT_14', 'PCIAT-PCIAT_15', \n           'PCIAT-PCIAT_16', 'PCIAT-PCIAT_17', 'PCIAT-PCIAT_18', 'PCIAT-PCIAT_19', \n           'PCIAT-PCIAT_20', 'PCIAT-PCIAT_Total']\n\ncolumns_to_drop = columns.copy()\n\n# Splitting data into unlabeled and labeled sets\nX_unlabeled = X_train[y_train.isna()].drop(columns=columns_to_drop)\nX_unlabeled = X_unlabeled.drop(columns='id')\nX_labeled = X_train[y_train.notna()].drop(columns='id')\ny_labeled = y_train[y_train.notna()].values\n\nprint('Target distribution:', pd.Series(y_labeled).value_counts())\n\n# Get extra features used for training\nextra_features_cols = [col for col in X_labeled.columns if col not in columns_to_drop]\nX_labeled.drop(columns=columns_to_drop, inplace=True)\n\n# Convert target labels to categorical\ny_labeled = to_categorical(y_labeled, num_classes=4)\n\n# Train models\nautoencoder, encoder, supervised_model, autoencoder_history, supervised_history = train_models(\n    X_unlabeled, X_labeled, y_labeled, extra_features_cols\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Importing essential libraries for data manipulation, analysis, and visualization\nimport numpy as np  # Library for numerical operations, useful for handling arrays and numerical data\nimport pandas as pd  # Library for data manipulation and analysis, ideal for working with DataFrame structures\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay  # Tools for evaluating model performance through confusion matrix\nimport matplotlib.pyplot as plt  # Library for creating static, interactive, and animated visualizations\n\ndef perform_inference(autoencoder, encoder, supervised_model, X_unseen, extra_features_cols, impute_strategy='median'):\n    \"\"\"\n    Perform inference using trained models (autoencoder, encoder, and supervised) on unseen data.\n    Preprocesses the data by imputing missing values and normalizing it, then makes predictions.\n\n    Parameters:\n    - autoencoder: Trained autoencoder model (used for reconstructive validation if needed).\n    - encoder: Encoder part of the autoencoder for feature extraction.\n    - supervised_model: Trained supervised model for classification.\n    - X_unseen: DataFrame containing unseen data (features) to make predictions on.\n    - extra_features_cols: List of column names representing extra features.\n    - impute_strategy: Strategy for imputing missing values ('median' by default).\n\n    Returns:\n    - y_pred_probs: Predicted probabilities for each class.\n    \"\"\"\n    # Split unseen data into base features and extra features based on column specification\n    X_unseen_base = X_unseen.drop(columns=extra_features_cols)  # Selecting base features\n    X_unseen_extra = X_unseen[extra_features_cols]  # Selecting extra features\n\n    # Impute missing values using the specified strategy (e.g., 'median')\n    imputer = SimpleImputer(strategy=impute_strategy)\n    X_unseen_base = imputer.fit_transform(X_unseen_base)\n    X_unseen_extra = imputer.fit_transform(X_unseen_extra)\n\n    # Normalize the data to scale values to a standard range (0-1 or standard score)\n    scaler = MinMaxScaler()  # MinMaxScaler ensures features are scaled between 0 and 1\n    X_unseen_base = scaler.fit_transform(X_unseen_base)\n    X_unseen_extra = scaler.fit_transform(X_unseen_extra)\n\n    # Use the encoder model to transform base features into encoded representations\n    X_unseen_encoded = encoder.predict(X_unseen_base)\n\n    # Optional: Check the shapes of the encoded and extra features to ensure compatibility\n    # assert X_unseen_encoded.shape[1] == 58, f\"Expected 58 features, got {X_unseen_encoded.shape[1]}\"\n    # assert X_unseen_extra.shape[1] == len(extra_features_cols), f\"Expected {len(extra_features_cols)} extra features, got {X_unseen_extra.shape[1]}\"\n\n    # Use the supervised model to predict probabilities for each class\n    y_pred_probs = supervised_model.predict([X_unseen_encoded, X_unseen_extra])\n\n    return y_pred_probs\n\n# Prepare test data by dropping the 'id' column for inference\nX_test_inference = X_test.drop(columns='id')\n\n# Example usage of the inference function\ny_pred_probs = perform_inference(\n    autoencoder=autoencoder,  # Pre-trained autoencoder model\n    encoder=encoder,  # Pre-trained encoder model\n    supervised_model=supervised_model,  # Pre-trained supervised model\n    X_unseen=X_test_inference,  # Unseen test data\n    extra_features_cols=columns  # List of columns representing extra features\n)\n\n# Display the predicted probabilities\nprint(y_pred_probs)\n\n# Convert the predicted probabilities to class labels using the argmax function\nclass_labels = np.argmax(y_pred_probs, axis=1)  # Axis 1 indicates row-wise operation for multi-class labels","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt  # Library for creating static and interactive visualizations\nimport seaborn as sns  # Library for advanced data visualization, used here to create heatmaps\nfrom sklearn.metrics import confusion_matrix, f1_score  # Metrics for evaluating classification models\n\n# Print the predicted class labels to verify output\nprint(\"Predicted class labels:\", class_labels)\n\n# Check if the actual labels (`y_test`) and predicted labels (`class_labels`) have the same length\nif len(y_test) == len(class_labels):\n    # Create a DataFrame to compare actual and predicted labels side-by-side\n    results_df = pd.DataFrame({\n        'Actual': y_test.values,        # Ensure `y_test` is converted to a NumPy array for consistency\n        'Predicted': class_labels       # Use the predicted class labels\n    })\n    # Print the DataFrame to display actual vs predicted labels\n    print(results_df)\nelse:\n    # Print a warning message if lengths do not match\n    print(\"Mismatch in length between actual and predicted labels.\")\n    \n# Filter out NaN values from `y_test` and `class_labels`\nvalid_indices = ~np.isnan(y_test) & ~np.isnan(class_labels)  # Create a mask for non-NaN indices\ny_test_clean = y_test[valid_indices]  # Apply the mask to filter `y_test`\nclass_labels_clean = class_labels[valid_indices]  # Apply the mask to filter `class_labels`\n\n# Check if the lengths of cleaned arrays match after filtering NaNs\nif len(y_test_clean) == len(class_labels_clean):\n    # Calculate the weighted F1 score to handle class imbalance in a multi-class setting\n    f1 = f1_score(y_test_clean, class_labels_clean, average='weighted')\n    print(f\"F1 Score (Weighted): {f1:.4f}\")  # Print the F1 score formatted to four decimal places\n    \n    # Compute the confusion matrix to evaluate model performance\n    cm = confusion_matrix(y_test_clean, class_labels_clean)\n\n    # Create a DataFrame for a more readable and visual representation of the confusion matrix\n    cm_df = pd.DataFrame(cm, \n                         index=[f'Class {i}' for i in range(cm.shape[0])],  # Create class labels for the rows\n                         columns=[f'Class {i}' for i in range(cm.shape[1])])  # Create class labels for the columns\n\n    # Plot the confusion matrix using Seaborn for better visualization\n    plt.figure(figsize=(10, 7))  # Set the figure size\n    sns.heatmap(cm_df, annot=True, fmt='d', cmap='Blues', cbar=False)  # Annotate cells with integer format, use a blue color map\n    plt.title('Confusion Matrix (Excluding NaN Values)')  # Title of the plot\n    plt.ylabel('Actual')  # Label for the y-axis\n    plt.xlabel('Predicted')  # Label for the x-axis\n    plt.show()  # Display the plot\nelse:\n    # Print a warning message if lengths do not match after filtering NaNs\n    print(\"Mismatch in length between actual and predicted labels after filtering NaNs.\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**SEMI SUPERVISED** with actigraphy","metadata":{}},{"cell_type":"code","source":"# Import necessary libraries for data manipulation, model training, and evaluation\nimport pandas as pd  # Library for data manipulation and analysis\nfrom sklearn.model_selection import train_test_split  # Utility for splitting data into training and testing sets\nfrom sklearn.linear_model import LinearRegression  # Linear regression model from scikit-learn\nfrom sklearn.metrics import mean_squared_error  # Metric to evaluate the performance of regression models\n\n# Load the training dataset into a DataFrame\ntrain_df = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/train.csv')  # Read CSV file into a DataFrame\n\n# Import necessary libraries from Keras for building neural network models\nfrom keras.models import Model  # Base class for defining a model in Keras\nfrom keras.layers import Input, Dense, Dropout  # Layers for creating the neural network architecture\n\n# Create a dictionary mapping season names to numerical values for encoding\nseason_mapping = {\n    'Fall': 0,    # Map 'Fall' to 0\n    'Winter': 1,  # Map 'Winter' to 1\n    'Spring': 2,  # Map 'Spring' to 2\n    'Summer': 3   # Map 'Summer' to 3\n}\n\n# Initialize an empty list to store column names related to 'Season'\nseason_columns = []\n\n# Iterate through all column names in the DataFrame\nfor col in train_df.keys():\n    # Check if 'Season' is in the column name\n    if 'Season' in col:\n        # Add the column name to the `season_columns` list\n        season_columns.append(col)\n\n# Print out the names of columns that contain 'Season'\nprint(season_columns)\n\n# Replace season values in the identified columns with the corresponding numerical values\ntrain_df[season_columns] = train_df[season_columns].replace(season_mapping)\n\n# Display the modified DataFrame\ntrain_df","metadata":{"execution":{"iopub.status.busy":"2024-11-06T11:07:26.814989Z","iopub.execute_input":"2024-11-06T11:07:26.815448Z","iopub.status.idle":"2024-11-06T11:07:44.546386Z","shell.execute_reply.started":"2024-11-06T11:07:26.815406Z","shell.execute_reply":"2024-11-06T11:07:44.544882Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"['Basic_Demos-Enroll_Season', 'CGAS-Season', 'Physical-Season', 'Fitness_Endurance-Season', 'FGC-Season', 'BIA-Season', 'PAQ_A-Season', 'PAQ_C-Season', 'PCIAT-Season', 'SDS-Season', 'PreInt_EduHx-Season']\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_31/2728716432.py:25: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n  train_df[season_columns] = train_df[season_columns].replace(season_mapping)\n","output_type":"stream"},{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"            id  Basic_Demos-Enroll_Season  Basic_Demos-Age  Basic_Demos-Sex  \\\n0     00008ff9                          0                5                0   \n1     000fd460                          3                9                0   \n2     00105258                          3               10                1   \n3     00115b9f                          1                9                0   \n4     0016bb22                          2               18                1   \n...        ...                        ...              ...              ...   \n3955  ff8a2de4                          0               13                0   \n3956  ffa9794a                          1               10                0   \n3957  ffcd4dbd                          0               11                0   \n3958  ffed1dd5                          2               13                0   \n3959  ffef538e                          2               11                0   \n\n      CGAS-Season  CGAS-CGAS_Score  Physical-Season  Physical-BMI  \\\n0             1.0             51.0              0.0     16.877316   \n1             NaN              NaN              0.0     14.035590   \n2             0.0             71.0              0.0     16.648696   \n3             0.0             71.0              3.0     18.292347   \n4             3.0              NaN              NaN           NaN   \n...           ...              ...              ...           ...   \n3955          2.0             60.0              0.0     16.362460   \n3956          NaN              NaN              2.0     18.764678   \n3957          2.0             68.0              1.0     21.441500   \n3958          2.0             70.0              1.0     12.235895   \n3959          NaN              NaN              1.0           NaN   \n\n      Physical-Height  Physical-Weight  ...  PCIAT-PCIAT_18  PCIAT-PCIAT_19  \\\n0                46.0             50.8  ...             4.0             2.0   \n1                48.0             46.0  ...             0.0             0.0   \n2                56.5             75.6  ...             2.0             1.0   \n3                56.0             81.6  ...             3.0             4.0   \n4                 NaN              NaN  ...             NaN             NaN   \n...               ...              ...  ...             ...             ...   \n3955             59.5             82.4  ...             1.0             1.0   \n3956             53.5             76.4  ...             NaN             NaN   \n3957             60.0            109.8  ...             1.0             0.0   \n3958             70.7             87.0  ...             1.0             1.0   \n3959              NaN              NaN  ...             NaN             NaN   \n\n      PCIAT-PCIAT_20  PCIAT-PCIAT_Total  SDS-Season  SDS-SDS_Total_Raw  \\\n0                4.0               55.0         NaN                NaN   \n1                0.0                0.0         0.0               46.0   \n2                1.0               28.0         0.0               38.0   \n3                1.0               44.0         3.0               31.0   \n4                NaN                NaN         NaN                NaN   \n...              ...                ...         ...                ...   \n3955             0.0               32.0         1.0               35.0   \n3956             NaN                NaN         NaN                NaN   \n3957             1.0               31.0         1.0               56.0   \n3958             1.0               19.0         2.0               33.0   \n3959             NaN                NaN         NaN                NaN   \n\n      SDS-SDS_Total_T  PreInt_EduHx-Season  \\\n0                 NaN                  0.0   \n1                64.0                  3.0   \n2                54.0                  3.0   \n3                45.0                  1.0   \n4                 NaN                  NaN   \n...               ...                  ...   \n3955             50.0                  0.0   \n3956              NaN                  1.0   \n3957             77.0                  0.0   \n3958             47.0                  2.0   \n3959              NaN                  2.0   \n\n      PreInt_EduHx-computerinternet_hoursday  sii  \n0                                        3.0  2.0  \n1                                        0.0  0.0  \n2                                        2.0  0.0  \n3                                        0.0  1.0  \n4                                        NaN  NaN  \n...                                      ...  ...  \n3955                                     1.0  1.0  \n3956                                     0.0  NaN  \n3957                                     0.0  1.0  \n3958                                     1.0  0.0  \n3959                                     1.0  NaN  \n\n[3960 rows x 82 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>Basic_Demos-Enroll_Season</th>\n      <th>Basic_Demos-Age</th>\n      <th>Basic_Demos-Sex</th>\n      <th>CGAS-Season</th>\n      <th>CGAS-CGAS_Score</th>\n      <th>Physical-Season</th>\n      <th>Physical-BMI</th>\n      <th>Physical-Height</th>\n      <th>Physical-Weight</th>\n      <th>...</th>\n      <th>PCIAT-PCIAT_18</th>\n      <th>PCIAT-PCIAT_19</th>\n      <th>PCIAT-PCIAT_20</th>\n      <th>PCIAT-PCIAT_Total</th>\n      <th>SDS-Season</th>\n      <th>SDS-SDS_Total_Raw</th>\n      <th>SDS-SDS_Total_T</th>\n      <th>PreInt_EduHx-Season</th>\n      <th>PreInt_EduHx-computerinternet_hoursday</th>\n      <th>sii</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>00008ff9</td>\n      <td>0</td>\n      <td>5</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>51.0</td>\n      <td>0.0</td>\n      <td>16.877316</td>\n      <td>46.0</td>\n      <td>50.8</td>\n      <td>...</td>\n      <td>4.0</td>\n      <td>2.0</td>\n      <td>4.0</td>\n      <td>55.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>3.0</td>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>000fd460</td>\n      <td>3</td>\n      <td>9</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>14.035590</td>\n      <td>48.0</td>\n      <td>46.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>46.0</td>\n      <td>64.0</td>\n      <td>3.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>00105258</td>\n      <td>3</td>\n      <td>10</td>\n      <td>1</td>\n      <td>0.0</td>\n      <td>71.0</td>\n      <td>0.0</td>\n      <td>16.648696</td>\n      <td>56.5</td>\n      <td>75.6</td>\n      <td>...</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>28.0</td>\n      <td>0.0</td>\n      <td>38.0</td>\n      <td>54.0</td>\n      <td>3.0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>00115b9f</td>\n      <td>1</td>\n      <td>9</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>71.0</td>\n      <td>3.0</td>\n      <td>18.292347</td>\n      <td>56.0</td>\n      <td>81.6</td>\n      <td>...</td>\n      <td>3.0</td>\n      <td>4.0</td>\n      <td>1.0</td>\n      <td>44.0</td>\n      <td>3.0</td>\n      <td>31.0</td>\n      <td>45.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0016bb22</td>\n      <td>2</td>\n      <td>18</td>\n      <td>1</td>\n      <td>3.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>3955</th>\n      <td>ff8a2de4</td>\n      <td>0</td>\n      <td>13</td>\n      <td>0</td>\n      <td>2.0</td>\n      <td>60.0</td>\n      <td>0.0</td>\n      <td>16.362460</td>\n      <td>59.5</td>\n      <td>82.4</td>\n      <td>...</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>32.0</td>\n      <td>1.0</td>\n      <td>35.0</td>\n      <td>50.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>3956</th>\n      <td>ffa9794a</td>\n      <td>1</td>\n      <td>10</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2.0</td>\n      <td>18.764678</td>\n      <td>53.5</td>\n      <td>76.4</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3957</th>\n      <td>ffcd4dbd</td>\n      <td>0</td>\n      <td>11</td>\n      <td>0</td>\n      <td>2.0</td>\n      <td>68.0</td>\n      <td>1.0</td>\n      <td>21.441500</td>\n      <td>60.0</td>\n      <td>109.8</td>\n      <td>...</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>31.0</td>\n      <td>1.0</td>\n      <td>56.0</td>\n      <td>77.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>3958</th>\n      <td>ffed1dd5</td>\n      <td>2</td>\n      <td>13</td>\n      <td>0</td>\n      <td>2.0</td>\n      <td>70.0</td>\n      <td>1.0</td>\n      <td>12.235895</td>\n      <td>70.7</td>\n      <td>87.0</td>\n      <td>...</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>19.0</td>\n      <td>2.0</td>\n      <td>33.0</td>\n      <td>47.0</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3959</th>\n      <td>ffef538e</td>\n      <td>2</td>\n      <td>11</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n<p>3960 rows × 82 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# Import necessary libraries for data manipulation and file handling\nimport numpy as np  # Library for numerical operations\nimport pandas as pd  # Library for data manipulation and analysis\nfrom sklearn.model_selection import train_test_split  # Utility to split data into training and test sets\nimport os  # Module to interact with the file system\n\n# Define the directory where actigraphy data (Parquet files) is located\ninput_directory = '/kaggle/input/child-mind-institute-problematic-internet-use/series_train.parquet'\n\n# Initialize an empty dictionary to store actigraphy data, keyed by participant ID\nactigraphy_data = {}\n\n# Iterate over the contents of the input directory\nfor subdir in os.listdir(input_directory):\n    # Check if the folder name starts with 'id=' (indicating it corresponds to a participant's data)\n    if subdir.startswith('id='):  \n        participant_id = subdir.split('=')[-1]  # Extract the participant ID from the folder name\n        \n        # Construct the path to the Parquet file for the participant\n        file_path = os.path.join(input_directory, subdir, 'part-0.parquet')\n        \n        # Check if the file exists\n        if os.path.exists(file_path):\n            # Read the Parquet file into a DataFrame and store it in the dictionary\n            df = pd.read_parquet(file_path)\n            actigraphy_data[participant_id] = df  # Store the DataFrame in the dictionary with the participant ID as the key\n\n# Assuming 'train_df' is the full dataset, separate the features (X) and target (y)\nX = train_df.drop(columns=['sii'])  # Drop the target variable 'sii' to create the feature set\ny = train_df['sii']  # Store the target variable 'sii' in the y variable\n\n# Create a set of participant IDs that have associated actigraphy data\nids_with_actigraphy = set(actigraphy_data.keys())\n\n# Add a new column to X that indicates whether a participant has actigraphy data\nX['has_actigraphy'] = X['id'].apply(lambda x: x in ids_with_actigraphy)\n\n# Perform a stratified train-test split, ensuring that the distribution of 'has_actigraphy' is preserved in both sets\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.1, random_state=42, stratify=X['has_actigraphy']\n)\n\n# Drop the 'has_actigraphy' column from the train and test sets, as it is no longer needed\nX_train = X_train.drop(columns=['has_actigraphy'])\nX_test = X_test.drop(columns=['has_actigraphy'])\n\n# Print the unique values in y_train to confirm that the stratification has been applied correctly\nprint(\"Unique values in y_train:\", np.unique(y_train))\n\n# Check if the test set contains participants with actigraphy data\nactigraphy_in_test = X_test['id'].apply(lambda x: x in ids_with_actigraphy)\n\n# Print the proportion of participants in the test set who have actigraphy data\nprint(f\"Proportion of actigraphy data in test set: {actigraphy_in_test.mean() * 100:.2f}%\")","metadata":{"execution":{"iopub.status.busy":"2024-11-06T11:07:55.753260Z","iopub.execute_input":"2024-11-06T11:07:55.753707Z","iopub.status.idle":"2024-11-06T11:09:05.045199Z","shell.execute_reply.started":"2024-11-06T11:07:55.753665Z","shell.execute_reply":"2024-11-06T11:09:05.043100Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Unique values in y_train: [ 0.  1.  2.  3. nan]\nProportion of actigraphy data in test set: 25.25%\n","output_type":"stream"}]},{"cell_type":"code","source":"def calculate_entropy(series):\n    \"\"\"Calculate entropy of activity patterns to measure regularity\"\"\"\n    hist, _ = np.histogram(series, bins=50)\n    prob = hist / hist.sum()\n    prob = prob[prob > 0]\n    return -np.sum(prob * np.log2(prob))\n\ndef calculate_sleep_features(data):\n    \"\"\"Extract sleep-related features based on activity and light patterns\"\"\"\n    # Define night hours (e.g., 10 PM to 6 AM)\n    night_mask = (data['hour'] >= 22) | (data['hour'] < 6)\n    night_data = data[night_mask]\n    \n    # Estimate sleep based on activity and light\n    sleep_periods = (night_data['enmo'] < 0.02) & (night_data['light'] < 5)\n    \n    return {\n        'sleep_duration': sleep_periods.mean() * 24,  # Approximate hours of sleep\n        'sleep_efficiency': sleep_periods.sum() / len(night_data),\n        'sleep_timing_variability': night_data['enmo'].std(),\n        'nocturnal_awakenings': count_awakenings(night_data['enmo'])\n    }\n\ndef analyze_circadian_rhythm(data):\n    \"\"\"Analyze circadian rhythm patterns\"\"\"\n    # Group by hour to get 24-hour pattern\n    hourly_activity = data.groupby('hour')['enmo'].mean()\n    \n    return {\n        'rhythm_stability': hourly_activity.std(),  # Lower values indicate more stable rhythm\n        'rhythm_amplitude': hourly_activity.max() - hourly_activity.min(),\n        'rhythm_acrophase': hourly_activity.idxmax()  # Hour of peak activity\n    }\n\ndef calculate_weekend_ratio(data):\n    \"\"\"Calculate ratio of weekend to weekday activity\"\"\"\n    weekend_mask = data['weekday'].isin([6, 7])\n    weekend_activity = data[weekend_mask]['enmo'].mean()\n    weekday_activity = data[~weekend_mask]['enmo'].mean()\n    return weekend_activity / (weekday_activity + 1e-6)\n\ndef calculate_evening_ratio(data):\n    \"\"\"Calculate ratio of evening (6PM-10PM) activity to daytime activity\"\"\"\n    evening_mask = (data['hour'] >= 18) & (data['hour'] < 22)\n    evening_activity = data[evening_mask]['enmo'].mean()\n    daytime_activity = data[~evening_mask]['enmo'].mean()\n    return evening_activity / (daytime_activity + 1e-6)\n\ndef calculate_late_night_activity(data):\n    \"\"\"Calculate activity levels during late night hours (11PM-4AM)\"\"\"\n    late_night = (data['hour'] >= 23) | (data['hour'] < 4)\n    return data[late_night]['enmo'].mean()\n\ndef calculate_sedentary_time(data):\n    \"\"\"Calculate proportion of time spent in sedentary behavior\"\"\"\n    return (data['enmo'] < 0.02).mean()\n\ndef calculate_activity_fragmentation(data):\n    \"\"\"Calculate how fragmented activity patterns are\"\"\"\n    # Define active state transitions\n    active = data['enmo'] > 0.1\n    transitions = np.diff(active.astype(int))\n    return np.sum(np.abs(transitions)) / len(data)\n\ndef count_active_periods(data):\n    \"\"\"Count distinct active periods per day\"\"\"\n    active = data['enmo'] > 0.1\n    transitions = np.diff(active.astype(int))\n    return np.sum(transitions > 0) / (len(data) / (24 * 60 * 12))  # Assuming 5-second epochs\n\ndef analyze_light_changes(data, period='night'):\n    \"\"\"Analyze frequency of light level changes during specified period\"\"\"\n    if period == 'night':\n        mask = (data['hour'] >= 22) | (data['hour'] < 6)\n    else:\n        mask = (data['hour'] >= 6) & (data['hour'] < 22)\n    \n    light_changes = np.diff(data[mask]['light']) != 0\n    return light_changes.mean()\n\ndef calculate_light_exposure(data, period='evening'):\n    \"\"\"Calculate light exposure during specified period\"\"\"\n    if period == 'evening':\n        mask = (data['hour'] >= 18) & (data['hour'] < 22)\n    else:\n        mask = (data['hour'] >= 6) & (data['hour'] < 18)\n    \n    return data[mask]['light'].mean()\n\ndef estimate_device_interaction(data):\n    \"\"\"Estimate potential device interaction based on light and activity patterns\"\"\"\n    night_mask = (data['hour'] >= 22) | (data['hour'] < 6)\n    night_data = data[night_mask]\n    \n    # Potential device use: low activity but moderate light levels\n    device_use = (night_data['enmo'] < 0.02) & (night_data['light'] > 5) & (night_data['light'] < 50)\n    return device_use.mean()\n\ndef count_activity_peaks(series):\n    \"\"\"Count number of significant activity peaks per day\"\"\"\n    threshold = series.mean() + 2 * series.std()\n    peaks = (series > threshold).astype(int)\n    return np.sum(np.diff(peaks) > 0) / (len(series) / (24 * 60 * 12))\n\ndef count_awakenings(night_activity):\n    \"\"\"Count number of potential awakenings during sleep period\"\"\"\n    # Define awakening as activity spike during night\n    threshold = night_activity.mean() + 1.5 * night_activity.std()\n    awakenings = (night_activity > threshold).astype(int)\n    return np.sum(np.diff(awakenings) > 0)\n\ndef calculate_activity_ratio(data):\n    \"\"\"Calculate ratio of active to inactive periods\"\"\"\n    active_threshold = data['enmo'].mean() + data['enmo'].std()\n    active_periods = (data['enmo'] > active_threshold).sum()\n    return active_periods / len(data) if len(data) > 0 else 0\n\ndef calculate_sleep_regularity(data):\n    \"\"\"Calculate sleep pattern regularity based on activity levels\"\"\"\n    night_hours = data['hour'].isin(range(22, 6))\n    night_activity = data.loc[night_hours, 'enmo'].mean()\n    day_activity = data.loc[~night_hours, 'enmo'].mean()\n    return 1 - (night_activity / (day_activity + 1e-6)) if day_activity > 0 else 0\n\ndef calculate_movement_intensity(data):\n    \"\"\"Calculate overall movement intensity\"\"\"\n    return np.percentile(data['enmo'], 95) if len(data) > 0 else 0","metadata":{"execution":{"iopub.status.busy":"2024-11-06T11:56:46.332307Z","iopub.execute_input":"2024-11-06T11:56:46.332746Z","iopub.status.idle":"2024-11-06T11:56:46.360603Z","shell.execute_reply.started":"2024-11-06T11:56:46.332706Z","shell.execute_reply":"2024-11-06T11:56:46.358847Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"def preprocess_data(X_unlabeled, X_labeled, extra_features_cols, actigraphy_data, is_training=True, normalize=True):\n    # Identify the base features from unlabeled data by excluding extra features\n    base_features = [col for col in X_unlabeled.columns if col not in extra_features_cols]\n    print(base_features)  # Debug print to verify selected base features\n\n    # Import the necessary imputer to handle missing values\n    from sklearn.impute import IterativeImputer\n\n    # Initialize the imputer using the IterativeImputer method (which uses a regression-based imputation)\n    imputer = IterativeImputer(random_state=42)\n\n    # Process the unlabeled data for autoencoder (to be used later for unsupervised learning)\n    X_unlabeled_base = X_unlabeled[base_features].copy()  # Keep only the base features\n    X_unlabeled_base_imputed = imputer.fit_transform(X_unlabeled_base)  # Impute missing values\n\n    # Process the labeled data for the supervised model\n    X_labeled_base = X_labeled[base_features].copy()  # Extract base features from labeled data\n    X_labeled_base_imputed = imputer.fit_transform(X_labeled_base)  # Impute missing values for base features\n\n    # Extract extra features from the labeled data (used for additional analysis or feature engineering)\n    X_labeled_extra = X_labeled[extra_features_cols].copy()  # Extract extra features\n    X_labeled_extra_imputed = imputer.fit_transform(X_labeled_extra)  # Impute missing values for extra features\n    \n    # Normalize data if specified\n    if normalize:\n        from sklearn.preprocessing import MinMaxScaler  # Import scaler for normalization\n        \n        # Normalize the imputed base features of both labeled and unlabeled data\n        scaler = MinMaxScaler()  # Use MinMaxScaler for normalization (could also use StandardScaler)\n        X_unlabeled_base = scaler.fit_transform(X_unlabeled_base_imputed)\n        X_labeled_base = scaler.fit_transform(X_labeled_base_imputed)\n        X_labeled_extra = scaler.fit_transform(X_labeled_extra_imputed)\n\n    # Convert the transformed data back into DataFrames with appropriate column names\n    X_unlabeled_base = pd.DataFrame(X_unlabeled_base, columns=base_features)\n    X_labeled_base = pd.DataFrame(X_labeled_base, columns=base_features)  # Transform base features into dataframe\n    X_labeled_extra = pd.DataFrame(X_labeled_extra, columns=extra_features_cols)  # Transform extra features into dataframe\n    \n    # Extract actigraphy features for the participants with actigraphy data\n    actigraphy_features = extract_actigraphy_features(actigraphy_data, X_labeled['id'].unique())  # Extract features for participants with actigraphy\n    # Merge the actigraphy features with the labeled data based on 'id'\n    X_labeled_actigraphy = pd.merge(X_labeled[['id']], actigraphy_features, on='id', how='left')\n\n    # Handle missing values in the actigraphy data\n    from sklearn.preprocessing import StandardScaler  # Import StandardScaler for actigraphy normalization\n    actigraphy_cols = X_labeled_actigraphy.columns.drop(['id', 'has_actigraphy'])  # Exclude non-relevant columns\n    X_labeled_actigraphy[actigraphy_cols] = scaler.fit_transform(\n        X_labeled_actigraphy[actigraphy_cols].fillna(X_labeled_actigraphy[actigraphy_cols].mean())  # Impute missing values and scale\n    )\n    \n    # Extract the 'has_actigraphy' flags to track whether each participant has actigraphy data\n    actigraphy_flags = X_labeled_actigraphy[['has_actigraphy']]\n    # Drop the 'id' and 'has_actigraphy' columns as they are no longer needed in the final dataset\n    X_labeled_actigraphy = X_labeled_actigraphy.drop(columns=['id', 'has_actigraphy'])\n    \n    # Debug print to check the shape of the processed actigraphy data\n    print('x labeled actigraphy', X_labeled_actigraphy.shape)\n\n    # Return the processed datasets depending on whether it's training or not\n    if is_training==True:\n        return X_labeled_extra, X_labeled_actigraphy, X_labeled_base, actigraphy_flags, X_unlabeled_base\n    else:\n        return X_labeled_extra, X_labeled_actigraphy, X_labeled_base, actigraphy_flags","metadata":{"execution":{"iopub.status.busy":"2024-11-06T12:04:21.839054Z","iopub.execute_input":"2024-11-06T12:04:21.839543Z","iopub.status.idle":"2024-11-06T12:04:21.852732Z","shell.execute_reply.started":"2024-11-06T12:04:21.839503Z","shell.execute_reply":"2024-11-06T12:04:21.851476Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom tensorflow.keras.layers import (\n    Input, Dense, Dropout, BatchNormalization, Concatenate, \n    Multiply, Reshape, Layer, LeakyReLU, GlobalAveragePooling1D,\n    Conv1D, MaxPooling1D, LayerNormalization\n)\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\nfrom tensorflow.keras.regularizers import l2\nfrom sklearn.impute import SimpleImputer\nfrom tensorflow.keras.utils import to_categorical\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\nimport tensorflow as tf\nfrom tensorflow.keras.optimizers import Adam\nfrom sklearn.experimental import enable_iterative_imputer\nfrom tensorflow.keras.metrics import AUC\n\n\nclass AttentionLayer(Layer):\n    def __init__(self, **kwargs):\n        super(AttentionLayer, self).__init__(**kwargs)\n        \n    def build(self, input_shape):\n        self.W = self.add_weight(name=\"att_weight\", shape=(input_shape[-1], 1),\n                                initializer=\"normal\")\n        self.b = self.add_weight(name=\"att_bias\", shape=(input_shape[1], 1),\n                                initializer=\"zeros\")\n        super(AttentionLayer, self).build(input_shape)\n        \n    def call(self, x):\n        # Proper attention mechanism using Keras operations\n        e = tf.keras.backend.dot(x, self.W) + self.b\n        a = tf.keras.backend.softmax(e, axis=1)\n        output = x * tf.keras.backend.repeat_elements(a, rep=x.shape[-1], axis=2)\n        return tf.keras.backend.sum(output, axis=1)\n\ndef extract_actigraphy_features_2(actigraphy_data, id_list):\n    \"\"\"\n    Extract relevant features from actigraphy time series data for each participant.\n    Handles missing data by filling NaN values with appropriate defaults.\n    \"\"\"\n    features = []  # List to store extracted features for each participant\n\n    for participant_id in id_list:\n        # Check if the participant's data exists in the dictionary\n        if participant_id in actigraphy_data:#if participant_id == '00115b9f':  #\n            # Get participant's data\n            participant_data = actigraphy_data[participant_id]\n\n            if not participant_data.empty:  # Check if the participant's data is not empty\n                # Fill NaN values with zeros or appropriate values\n                participant_data = participant_data.fillna(0)\n\n                # Convert time_of_day to datetime\n                participant_data['time'] = pd.to_datetime(participant_data['time_of_day'])\n                participant_data['hour'] = participant_data['time'].dt.hour\n                \n                # 1. Activity Patterns\n                enmo_stats = {\n                    'enmo_mean': participant_data['enmo'].mean(),\n                    'enmo_std': participant_data['enmo'].std(),\n                    'enmo_max': participant_data['enmo'].max(),\n                    'enmo_min': participant_data['enmo'].min(),\n                    'enmo_median': participant_data['enmo'].median(),\n                    'activity_entropy': calculate_entropy(participant_data['enmo']),\n                    'activity_peaks_per_day': count_activity_peaks(participant_data['enmo'])\n                }\n                \n                # 2. Sleep Pattern Features\n                sleep_features = calculate_sleep_features(participant_data)\n                \n                # 3. Circadian Rhythm Features\n                circadian_features = analyze_circadian_rhythm(participant_data)\n                \n                # 4. Daily Pattern Features\n                daily_features = {\n                    'weekend_activity_ratio': calculate_weekend_ratio(participant_data),\n                    'evening_activity_ratio': calculate_evening_ratio(participant_data),\n                    'late_night_activity': calculate_late_night_activity(participant_data)\n                }\n                \n                # 5. Physical Activity Features\n                physical_features = {\n                    'sedentary_time_ratio': calculate_sedentary_time(participant_data),\n                    'activity_fragmentation': calculate_activity_fragmentation(participant_data),\n                    'active_periods_per_day': count_active_periods(participant_data)\n                }\n                \n                # 6. Device Interaction Features\n                interaction_features = {\n                    'light_changes_night': analyze_light_changes(participant_data, period='night'),\n                    'light_exposure_evening': calculate_light_exposure(participant_data, period='evening'),\n                    'device_interaction_night': estimate_device_interaction(participant_data)\n                }\n                \n                has_actigraphy = 1.0\n            else:\n                # Default values for empty data\n                has_actigraphy = 0.0\n                enmo_stats = {k: 0 for k in ['enmo_mean', 'enmo_std', 'enmo_max', 'enmo_min', 'enmo_median', \n                                           'activity_entropy', 'activity_peaks_per_day']}\n                sleep_features = {k: 0 for k in ['sleep_duration', 'sleep_efficiency', 'sleep_timing_variability', \n                                               'nocturnal_awakenings']}\n                circadian_features = {k: 0 for k in ['rhythm_stability', 'rhythm_amplitude', 'rhythm_acrophase']}\n                activity_pattern = {k: 0 for k in ['night_activity', 'day_activity', 'activity_ratio', 'non_wear_ratio']}\n                daily_features = {k: 0 for k in ['weekend_activity_ratio', 'evening_activity_ratio', 'late_night_activity']}\n                physical_features = {k: 0 for k in ['sedentary_time_ratio', 'activity_fragmentation', 'active_periods_per_day']}\n                interaction_features = {k: 0 for k in ['light_changes_night', 'light_exposure_evening', 'device_interaction_night']}\n        \n        else:\n            # Default values for missing data\n            has_actigraphy = 0.0\n            enmo_stats = {k: 0 for k in ['enmo_mean', 'enmo_std', 'enmo_max', 'enmo_min', 'enmo_median', \n                                       'activity_entropy', 'activity_peaks_per_day']}\n            sleep_features = {k: 0 for k in ['sleep_duration', 'sleep_efficiency', 'sleep_timing_variability', \n                                           'nocturnal_awakenings']}\n            circadian_features = {k: 0 for k in ['rhythm_stability', 'rhythm_amplitude', 'rhythm_acrophase']}\n            activity_pattern = {k: 0 for k in ['night_activity', 'day_activity', 'activity_ratio', 'non_wear_ratio']}\n            daily_features = {k: 0 for k in ['weekend_activity_ratio', 'evening_activity_ratio', 'late_night_activity']}\n            physical_features = {k: 0 for k in ['sedentary_time_ratio', 'activity_fragmentation', 'active_periods_per_day']}\n            interaction_features = {k: 0 for k in ['light_changes_night', 'light_exposure_evening', 'device_interaction_night']}\n\n        '''participant_features = {\n            'id': participant_id,\n            'has_actigraphy': has_actigraphy,\n            **activity_features,\n            **sleep_features,\n            **circadian_features,\n            **daily_features,\n            **physical_features,\n            **interaction_features\n        }'''\n        \n        participant_features = {\n            'id': participant_id,\n            'has_actigraphy': has_actigraphy,\n            **enmo_stats,\n            **sleep_features,\n            **circadian_features,\n            **activity_pattern,\n            **daily_features,\n            **physical_features,\n            #**interaction_features\n        }\n        \n        features.append(participant_features)\n\n    # Convert list to a DataFrame and handle any NaN values by filling them with zeros\n    features_df = pd.DataFrame(features).fillna(0)\n    print('features df shape', features_df.shape)\n\n    return features_df  # Return the DataFrame\n\ndef create_adaptive_model(base_input_dim, extra_features_dim, actigraphy_features_dim, encoding_dim=32, dropout_rate=0.3):\n    \"\"\"\n    Creates a model that can handle both participants with and without actigraphy data.\n    \"\"\"\n    # Base input for original features\n    base_input = Input(shape=(base_input_dim,), name='base_input')\n    \n    def encoder_block(x, units, dropout_rate=0.3):\n        skip = x\n        x = Dense(units)(x)\n        x = LeakyReLU(alpha=0.2)(x)\n        x = BatchNormalization()(x)\n        x = Dropout(dropout_rate)(x)\n        \n        if int(skip.shape[-1]) != units:\n            skip = Dense(units)(skip)\n        x = tf.keras.layers.Add()([x, skip])\n        return x\n    \n    # Encoder layers\n    encoded = encoder_block(base_input, 256)\n    encoded = encoder_block(encoded, 192)\n    encoded = encoder_block(encoded, encoding_dim)\n    \n    # Reshape for attention layer\n    encoded_reshaped = Reshape((1, encoding_dim))(encoded)\n    #attention_output = MultiHeadAttention(num_heads=8, head_size=8)(encoded_reshaped)\n    #attention = GlobalAveragePooling1D()(attention_output)\n    \n    # Apply attention\n    attention = AttentionLayer()(encoded_reshaped)\n    \n    # Decoder with similar architecture\n    decoded = encoder_block(attention, 192)\n    decoded = encoder_block(decoded, 256)\n    decoder_output = Dense(base_input_dim, activation='sigmoid')(decoded)\n    \n    # Create autoencoder and encoder models\n    autoencoder = Model(inputs=base_input, outputs=decoder_output)\n    encoder = Model(inputs=base_input, outputs=encoded)  # Modified to output encoded features directly\n    \n    # Create separate input for supervised model that matches encoder output\n    encoded_input = Input(shape=(encoding_dim,), name='encoded_input')  # Modified to match encoder output\n    extra_input = Input(shape=(extra_features_dim,), name='extra_input')\n    actigraphy_input = Input(shape=(actigraphy_features_dim - 1,), name='actigraphy_input')\n    \n    print('arctigraphy input shape', actigraphy_input.shape)\n    actigraphy_flag = Input(shape=(1,), name='actigraphy_flag')\n\n    # Process actigraphy features\n    actigraphy_features = Dense(48, activation='relu')(actigraphy_input)\n    actigraphy_features = BatchNormalization()(actigraphy_features)\n    actigraphy_features_reshaped = Reshape((1, 48))(actigraphy_features)\n    actigraphy_attention = AttentionLayer()(actigraphy_features_reshaped)\n    actigraphy_weighted = Multiply()([actigraphy_attention, actigraphy_flag])\n    \n    # Concatenate all feature inputs\n    combined = Concatenate()([encoded_input, extra_input, actigraphy_weighted])\n\n    # Supervised pathway\n    supervised = encoder_block(combined, 512)\n    supervised = encoder_block(combined, 384)\n    supervised = encoder_block(supervised, 256)\n    supervised = encoder_block(supervised, 128)\n    \n    # Final classification layers\n    #pre_output = Dense(4)(supervised)\n    output = Dense(4, activation='softmax')(supervised)\n    \n    supervised_model = Model(\n        inputs=[encoded_input, extra_input, actigraphy_input, actigraphy_flag],\n        outputs=output\n    )\n\n    return autoencoder, encoder, supervised_model  # Return all models\n\n# Import necessary libraries if not already imported\nfrom tensorflow.keras.utils import to_categorical\n\ndef train_adaptive_models(X_unlabeled, X_labeled, y_labeled, extra_features_cols, actigraphy_data, common_ids, encoding_dim=32):\n    \"\"\"\n    Train models that adapt to the presence/absence of actigraphy data.\n    \"\"\"\n    # Convert labels to one-hot encoding for classification\n    y_labeled_encoded = to_categorical(np.round(y_labeled).astype(np.int64))  # Convert to int before one-hot encoding\n    \n    X_labeled_extra, X_labeled_actigraphy, X_labeled_base, actigraphy_flags, X_unlabeled_base=preprocess_data(X_unlabeled, X_labeled, extra_features_cols, \n                                                                                              actigraphy_data, is_training=True, normalize=True)\n    \n    # Create the autoencoder and supervised models\n    autoencoder, encoder, supervised_model = create_adaptive_model(\n        base_input_dim=len(X_unlabeled_base.columns),\n        extra_features_dim=len(extra_features_cols),\n        actigraphy_features_dim=len(X_labeled_actigraphy.columns) + 1,  # Include the actigraphy flag\n        encoding_dim=encoding_dim\n    ) #58 + 22 + number of actigraphy features\n    \n    # Compile the autoencoder with a mean squared error loss for unsupervised training\n    autoencoder.compile(\n        optimizer=tf.keras.optimizers.AdamW(\n            learning_rate=0.001,\n            weight_decay=0.01\n        ),\n        loss='mean_squared_error'\n    )\n    \n    supervised_model.compile(\n        optimizer=tf.keras.optimizers.AdamW(\n            learning_rate=0.0005,\n            weight_decay=0.01\n        ),\n        loss='categorical_crossentropy', #custom_loss,\n        metrics=['accuracy', AUC(name='auc')]\n    )\n\n    # Train the autoencoder on the unlabeled base features\n    autoencoder_history = autoencoder.fit(\n        X_unlabeled_base, X_unlabeled_base,\n        epochs=200,\n        batch_size=32,\n        validation_split=0.15,\n        callbacks=[\n            EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True),\n            ReduceLROnPlateau(factor=0.5, patience=7, min_lr=1e-6),\n            tf.keras.callbacks.ModelCheckpoint(\n                'best_autoencoder.keras',  # Changed from .h5 to .keras\n                save_best_only=True,\n                monitor='val_loss'\n            )\n        ]\n    )\n    \n    # Obtain encoded features using the encoder part of the autoencoder for the labeled dataset\n    encoded_features = encoder.predict(X_labeled_base)\n    \n    class_weights = compute_class_weights(y_labeled)\n    #class_weights={0: 0.4273356401384083, 1: 0.9398782343987824, 2: 1.8269230769230769, 3: 10.583333333333332}\n    \n    print(\"Class weights:\", class_weights)  # Debug print to verify weights\n    \n    print(np.isnan(encoded_features).any(), X_labeled_extra.isnull().values.any(), X_labeled_actigraphy.isnull().values.any(),\n         actigraphy_flags.isnull().values.any()) #X_labeled_actigraphy has nan values\n\n    # Train the supervised model using the encoded features and additional inputs\n    supervised_history = supervised_model.fit(\n        [encoded_features, X_labeled_extra, X_labeled_actigraphy, actigraphy_flags],\n        to_categorical(np.round(y_labeled).astype(np.int64)),\n        epochs=300,\n        batch_size=32,\n        validation_split=0.15,\n        class_weight=class_weights,\n        callbacks=[\n            EarlyStopping(monitor='val_auc', mode='max', patience=20, restore_best_weights=True),\n            ReduceLROnPlateau(monitor='val_auc', mode='max', factor=0.5, patience=10, min_lr=1e-6),\n            tf.keras.callbacks.ModelCheckpoint(\n                'best_supervised.keras',  # Changed from .h5 to .keras\n                save_best_only=True,\n                monitor='val_auc',\n                mode='max'\n            )\n        ]\n    )\n\n    # Return trained models and training histories for analysis\n    return autoencoder, encoder, supervised_model, autoencoder_history, supervised_history\n\ndef compute_class_weights(y):\n    \"\"\"\n    Compute balanced class weights with proper type handling\n    \n    Parameters:\n    y : array-like\n        Target values, can be float or int\n        \n    Returns:\n    dict\n        Dictionary mapping class indices to class weights\n    \"\"\"\n    # Convert to numpy array if not already\n    y = np.array(y)\n    \n    # Check if values are floats and convert to int\n    if y.dtype in [np.float32, np.float64]:\n        # Round to nearest integer and convert to int\n        y = np.round(y).astype(np.int64)\n    \n    # Get unique classes and their counts\n    unique_classes, class_counts = np.unique(y, return_counts=True)\n    \n    # Compute weights\n    total = len(y)\n    n_classes = len(unique_classes)\n    weights = {int(cls): total / (n_classes * count) for cls, count in zip(unique_classes, class_counts)}\n    \n    return weights\n\n# Usage:\n\ncolumns=['PCIAT-Season', 'PCIAT-PCIAT_01', 'PCIAT-PCIAT_02', \n                   'PCIAT-PCIAT_03', 'PCIAT-PCIAT_04', 'PCIAT-PCIAT_05', \n                   'PCIAT-PCIAT_06', 'PCIAT-PCIAT_07', 'PCIAT-PCIAT_08', \n                   'PCIAT-PCIAT_09', 'PCIAT-PCIAT_10', 'PCIAT-PCIAT_11', \n                   'PCIAT-PCIAT_12', 'PCIAT-PCIAT_13', 'PCIAT-PCIAT_14', \n                   'PCIAT-PCIAT_15', 'PCIAT-PCIAT_16', 'PCIAT-PCIAT_17', \n                   'PCIAT-PCIAT_18', 'PCIAT-PCIAT_19', 'PCIAT-PCIAT_20', \n                   'PCIAT-PCIAT_Total']\n\ncolumns_to_drop = columns.copy()\n\nimport os\n# List all available IDs from the actigraphy data folder as strings\nactigraphy_ids = [filename.split('=')[1].split('.')[0] for filename in os.listdir('/kaggle/input/child-mind-institute-problematic-internet-use/series_train.parquet')]\n# Ensure the 'id' column in train_df is treated as a string\ntrain_df['id'] = train_df['id'].astype(str)\n# Find common IDs by treating both as strings\ncommon_ids = set(train_df['id']).intersection(set(actigraphy_ids))\nprint(f\"Number of participants with actigraphy data: {len(common_ids)}\")\n\n# Filter actigraphy data for participants in the training set\ntrain_actigraphy_data = {pid: actigraphy_data[pid] for pid in X_train['id'] if pid in actigraphy_data}\n\n# Check the number of participants with actigraphy data in the training set\nprint(f\"Number of participants with actigraphy data in training set: {len(train_actigraphy_data)}\")\n\nX_unlabeled = X_train[y_train.isna()].drop(columns=columns_to_drop)\nX_unlabeled = X_unlabeled.drop(columns='id')\nX_labeled = X_train[y_train.notna()]\ny_labeled = y_train[y_train.notna()].values\n\n#actigraphy_data=dataframes.copy()\n#X_unlabeled is the data that does not have a sii\n#X labeled has sii \n\nautoencoder, encoder, supervised_model, autoencoder_history, supervised_history = train_adaptive_models(\n    X_unlabeled, \n    X_labeled, \n    y_labeled, \n    columns, \n    train_actigraphy_data, \n    common_ids, \n    encoding_dim=32\n)\n\n# Save the trained autoencoder model\nautoencoder.save('autoencoder_model.h5')\nprint(\"Autoencoder model saved as 'autoencoder_model.h5'\")\n# Save the trained supervised model\nsupervised_model.save('supervised_model.h5')\nprint(\"Supervised model saved as 'supervised_model.h5'\")","metadata":{"execution":{"iopub.status.busy":"2024-11-06T12:04:25.789467Z","iopub.execute_input":"2024-11-06T12:04:25.789917Z","iopub.status.idle":"2024-11-06T12:07:29.879102Z","shell.execute_reply.started":"2024-11-06T12:04:25.789877Z","shell.execute_reply":"2024-11-06T12:07:29.877627Z"},"trusted":true},"execution_count":40,"outputs":[{"name":"stdout","text":"Number of participants with actigraphy data: 996\nNumber of participants with actigraphy data in training set: 896\n['Basic_Demos-Enroll_Season', 'Basic_Demos-Age', 'Basic_Demos-Sex', 'CGAS-Season', 'CGAS-CGAS_Score', 'Physical-Season', 'Physical-BMI', 'Physical-Height', 'Physical-Weight', 'Physical-Waist_Circumference', 'Physical-Diastolic_BP', 'Physical-HeartRate', 'Physical-Systolic_BP', 'Fitness_Endurance-Season', 'Fitness_Endurance-Max_Stage', 'Fitness_Endurance-Time_Mins', 'Fitness_Endurance-Time_Sec', 'FGC-Season', 'FGC-FGC_CU', 'FGC-FGC_CU_Zone', 'FGC-FGC_GSND', 'FGC-FGC_GSND_Zone', 'FGC-FGC_GSD', 'FGC-FGC_GSD_Zone', 'FGC-FGC_PU', 'FGC-FGC_PU_Zone', 'FGC-FGC_SRL', 'FGC-FGC_SRL_Zone', 'FGC-FGC_SRR', 'FGC-FGC_SRR_Zone', 'FGC-FGC_TL', 'FGC-FGC_TL_Zone', 'BIA-Season', 'BIA-BIA_Activity_Level_num', 'BIA-BIA_BMC', 'BIA-BIA_BMI', 'BIA-BIA_BMR', 'BIA-BIA_DEE', 'BIA-BIA_ECW', 'BIA-BIA_FFM', 'BIA-BIA_FFMI', 'BIA-BIA_FMI', 'BIA-BIA_Fat', 'BIA-BIA_Frame_num', 'BIA-BIA_ICW', 'BIA-BIA_LDM', 'BIA-BIA_LST', 'BIA-BIA_SMM', 'BIA-BIA_TBW', 'PAQ_A-Season', 'PAQ_A-PAQ_A_Total', 'PAQ_C-Season', 'PAQ_C-PAQ_C_Total', 'SDS-Season', 'SDS-SDS_Total_Raw', 'SDS-SDS_Total_T', 'PreInt_EduHx-Season', 'PreInt_EduHx-computerinternet_hoursday']\nx labeled actigraphy (2470, 6)\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/layers/activations/leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"arctigraphy input shape (None, 6)\nEpoch 1/200\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 26ms/step - loss: 0.1080 - val_loss: 0.0403 - learning_rate: 0.0010\nEpoch 2/200\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0475 - val_loss: 0.0260 - learning_rate: 0.0010\nEpoch 3/200\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0398 - val_loss: 0.0190 - learning_rate: 0.0010\nEpoch 4/200\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0323 - val_loss: 0.0142 - learning_rate: 0.0010\nEpoch 5/200\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0274 - val_loss: 0.0186 - learning_rate: 0.0010\nEpoch 6/200\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0260 - val_loss: 0.0152 - learning_rate: 0.0010\nEpoch 7/200\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0202 - val_loss: 0.0138 - learning_rate: 0.0010\nEpoch 8/200\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0189 - val_loss: 0.0115 - learning_rate: 0.0010\nEpoch 9/200\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0188 - val_loss: 0.0094 - learning_rate: 0.0010\nEpoch 10/200\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0171 - val_loss: 0.0095 - learning_rate: 0.0010\nEpoch 11/200\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0164 - val_loss: 0.0083 - learning_rate: 0.0010\nEpoch 12/200\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0146 - val_loss: 0.0070 - learning_rate: 0.0010\nEpoch 13/200\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0139 - val_loss: 0.0064 - learning_rate: 0.0010\nEpoch 14/200\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0129 - val_loss: 0.0057 - learning_rate: 0.0010\nEpoch 15/200\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0125 - val_loss: 0.0054 - learning_rate: 0.0010\nEpoch 16/200\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0125 - val_loss: 0.0049 - learning_rate: 0.0010\nEpoch 17/200\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0121 - val_loss: 0.0056 - learning_rate: 0.0010\nEpoch 18/200\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0115 - val_loss: 0.0042 - learning_rate: 0.0010\nEpoch 19/200\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0104 - val_loss: 0.0039 - learning_rate: 0.0010\nEpoch 20/200\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0097 - val_loss: 0.0041 - learning_rate: 0.0010\nEpoch 21/200\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0100 - val_loss: 0.0039 - learning_rate: 0.0010\nEpoch 22/200\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0091 - val_loss: 0.0035 - learning_rate: 0.0010\nEpoch 23/200\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0089 - val_loss: 0.0035 - learning_rate: 0.0010\nEpoch 24/200\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0094 - val_loss: 0.0039 - learning_rate: 0.0010\nEpoch 25/200\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0082 - val_loss: 0.0032 - learning_rate: 0.0010\nEpoch 26/200\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0082 - val_loss: 0.0034 - learning_rate: 0.0010\nEpoch 27/200\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0076 - val_loss: 0.0031 - learning_rate: 0.0010\nEpoch 28/200\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0078 - val_loss: 0.0027 - learning_rate: 0.0010\nEpoch 29/200\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0068 - val_loss: 0.0027 - learning_rate: 0.0010\nEpoch 30/200\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0076 - val_loss: 0.0027 - learning_rate: 0.0010\nEpoch 31/200\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0068 - val_loss: 0.0030 - learning_rate: 0.0010\nEpoch 32/200\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0065 - val_loss: 0.0032 - learning_rate: 0.0010\nEpoch 33/200\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0063 - val_loss: 0.0027 - learning_rate: 0.0010\nEpoch 34/200\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0062 - val_loss: 0.0027 - learning_rate: 0.0010\nEpoch 35/200\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0063 - val_loss: 0.0029 - learning_rate: 0.0010\nEpoch 36/200\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0057 - val_loss: 0.0021 - learning_rate: 5.0000e-04\nEpoch 37/200\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0057 - val_loss: 0.0022 - learning_rate: 5.0000e-04\nEpoch 38/200\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0049 - val_loss: 0.0019 - learning_rate: 5.0000e-04\nEpoch 39/200\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0049 - val_loss: 0.0017 - learning_rate: 5.0000e-04\nEpoch 40/200\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0051 - val_loss: 0.0018 - learning_rate: 5.0000e-04\nEpoch 41/200\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0047 - val_loss: 0.0017 - learning_rate: 5.0000e-04\nEpoch 42/200\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0049 - val_loss: 0.0018 - learning_rate: 5.0000e-04\nEpoch 43/200\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0045 - val_loss: 0.0016 - learning_rate: 5.0000e-04\nEpoch 44/200\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0048 - val_loss: 0.0016 - learning_rate: 5.0000e-04\nEpoch 45/200\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0044 - val_loss: 0.0016 - learning_rate: 5.0000e-04\nEpoch 46/200\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0043 - val_loss: 0.0017 - learning_rate: 5.0000e-04\nEpoch 47/200\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0045 - val_loss: 0.0014 - learning_rate: 2.5000e-04\nEpoch 48/200\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0042 - val_loss: 0.0014 - learning_rate: 2.5000e-04\nEpoch 49/200\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0040 - val_loss: 0.0014 - learning_rate: 2.5000e-04\nEpoch 50/200\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0036 - val_loss: 0.0013 - learning_rate: 2.5000e-04\nEpoch 51/200\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0037 - val_loss: 0.0014 - learning_rate: 2.5000e-04\nEpoch 52/200\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0039 - val_loss: 0.0013 - learning_rate: 2.5000e-04\nEpoch 53/200\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0037 - val_loss: 0.0014 - learning_rate: 2.5000e-04\nEpoch 54/200\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0039 - val_loss: 0.0013 - learning_rate: 2.5000e-04\nEpoch 55/200\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0036 - val_loss: 0.0012 - learning_rate: 1.2500e-04\nEpoch 56/200\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0036 - val_loss: 0.0012 - learning_rate: 1.2500e-04\nEpoch 57/200\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0035 - val_loss: 0.0012 - learning_rate: 1.2500e-04\nEpoch 58/200\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0036 - val_loss: 0.0012 - learning_rate: 1.2500e-04\nEpoch 59/200\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0036 - val_loss: 0.0012 - learning_rate: 1.2500e-04\nEpoch 60/200\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0035 - val_loss: 0.0012 - learning_rate: 1.2500e-04\nEpoch 61/200\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0035 - val_loss: 0.0012 - learning_rate: 1.2500e-04\nEpoch 62/200\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0033 - val_loss: 0.0012 - learning_rate: 1.2500e-04\nEpoch 63/200\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0033 - val_loss: 0.0012 - learning_rate: 6.2500e-05\nEpoch 64/200\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0034 - val_loss: 0.0011 - learning_rate: 6.2500e-05\nEpoch 65/200\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0033 - val_loss: 0.0011 - learning_rate: 6.2500e-05\nEpoch 66/200\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0037 - val_loss: 0.0011 - learning_rate: 6.2500e-05\nEpoch 67/200\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0034 - val_loss: 0.0012 - learning_rate: 6.2500e-05\nEpoch 68/200\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0034 - val_loss: 0.0011 - learning_rate: 6.2500e-05\nEpoch 69/200\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0034 - val_loss: 0.0011 - learning_rate: 6.2500e-05\nEpoch 70/200\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0032 - val_loss: 0.0011 - learning_rate: 6.2500e-05\nEpoch 71/200\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0034 - val_loss: 0.0011 - learning_rate: 6.2500e-05\nEpoch 72/200\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0033 - val_loss: 0.0011 - learning_rate: 3.1250e-05\nEpoch 73/200\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0033 - val_loss: 0.0011 - learning_rate: 3.1250e-05\nEpoch 74/200\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0032 - val_loss: 0.0011 - learning_rate: 3.1250e-05\nEpoch 75/200\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0036 - val_loss: 0.0011 - learning_rate: 3.1250e-05\nEpoch 76/200\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0032 - val_loss: 0.0011 - learning_rate: 3.1250e-05\nEpoch 77/200\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0033 - val_loss: 0.0011 - learning_rate: 3.1250e-05\nEpoch 78/200\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0033 - val_loss: 0.0011 - learning_rate: 3.1250e-05\nEpoch 79/200\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0032 - val_loss: 0.0011 - learning_rate: 1.5625e-05\nEpoch 80/200\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0034 - val_loss: 0.0011 - learning_rate: 1.5625e-05\nEpoch 81/200\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0031 - val_loss: 0.0011 - learning_rate: 1.5625e-05\nEpoch 82/200\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0034 - val_loss: 0.0011 - learning_rate: 1.5625e-05\nEpoch 83/200\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0031 - val_loss: 0.0011 - learning_rate: 1.5625e-05\nEpoch 84/200\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0032 - val_loss: 0.0011 - learning_rate: 1.5625e-05\nEpoch 85/200\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0032 - val_loss: 0.0011 - learning_rate: 1.5625e-05\nEpoch 86/200\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0031 - val_loss: 0.0011 - learning_rate: 7.8125e-06\nEpoch 87/200\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0031 - val_loss: 0.0011 - learning_rate: 7.8125e-06\nEpoch 88/200\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0034 - val_loss: 0.0011 - learning_rate: 7.8125e-06\nEpoch 89/200\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0033 - val_loss: 0.0011 - learning_rate: 7.8125e-06\nEpoch 90/200\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0031 - val_loss: 0.0011 - learning_rate: 7.8125e-06\nEpoch 91/200\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0034 - val_loss: 0.0011 - learning_rate: 7.8125e-06\nEpoch 92/200\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0032 - val_loss: 0.0011 - learning_rate: 7.8125e-06\nEpoch 93/200\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0032 - val_loss: 0.0011 - learning_rate: 3.9063e-06\nEpoch 94/200\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0033 - val_loss: 0.0011 - learning_rate: 3.9063e-06\nEpoch 95/200\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0032 - val_loss: 0.0011 - learning_rate: 3.9063e-06\nEpoch 96/200\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0031 - val_loss: 0.0011 - learning_rate: 3.9063e-06\nEpoch 97/200\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0033 - val_loss: 0.0011 - learning_rate: 3.9063e-06\nEpoch 98/200\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0032 - val_loss: 0.0011 - learning_rate: 3.9063e-06\nEpoch 99/200\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0032 - val_loss: 0.0011 - learning_rate: 3.9063e-06\nEpoch 100/200\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0031 - val_loss: 0.0011 - learning_rate: 1.9531e-06\nEpoch 101/200\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0031 - val_loss: 0.0011 - learning_rate: 1.9531e-06\nEpoch 102/200\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0033 - val_loss: 0.0011 - learning_rate: 1.9531e-06\nEpoch 103/200\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0033 - val_loss: 0.0011 - learning_rate: 1.9531e-06\nEpoch 104/200\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0031 - val_loss: 0.0011 - learning_rate: 1.9531e-06\nEpoch 105/200\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0029 - val_loss: 0.0011 - learning_rate: 1.9531e-06\nEpoch 106/200\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0032 - val_loss: 0.0011 - learning_rate: 1.9531e-06\nEpoch 107/200\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0033 - val_loss: 0.0011 - learning_rate: 1.0000e-06\nEpoch 108/200\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0030 - val_loss: 0.0011 - learning_rate: 1.0000e-06\nEpoch 109/200\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0031 - val_loss: 0.0011 - learning_rate: 1.0000e-06\nEpoch 110/200\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0035 - val_loss: 0.0011 - learning_rate: 1.0000e-06\nEpoch 111/200\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0031 - val_loss: 0.0011 - learning_rate: 1.0000e-06\nEpoch 112/200\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0034 - val_loss: 0.0011 - learning_rate: 1.0000e-06\nEpoch 113/200\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0030 - val_loss: 0.0011 - learning_rate: 1.0000e-06\nEpoch 114/200\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0033 - val_loss: 0.0011 - learning_rate: 1.0000e-06\nEpoch 115/200\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0033 - val_loss: 0.0011 - learning_rate: 1.0000e-06\nEpoch 116/200\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0035 - val_loss: 0.0011 - learning_rate: 1.0000e-06\nEpoch 117/200\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0032 - val_loss: 0.0011 - learning_rate: 1.0000e-06\nEpoch 118/200\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0031 - val_loss: 0.0011 - learning_rate: 1.0000e-06\nEpoch 119/200\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0034 - val_loss: 0.0011 - learning_rate: 1.0000e-06\nEpoch 120/200\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0031 - val_loss: 0.0011 - learning_rate: 1.0000e-06\nEpoch 121/200\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0032 - val_loss: 0.0011 - learning_rate: 1.0000e-06\nEpoch 122/200\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0033 - val_loss: 0.0011 - learning_rate: 1.0000e-06\nEpoch 123/200\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0029 - val_loss: 0.0011 - learning_rate: 1.0000e-06\nEpoch 124/200\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0032 - val_loss: 0.0011 - learning_rate: 1.0000e-06\nEpoch 125/200\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0033 - val_loss: 0.0011 - learning_rate: 1.0000e-06\nEpoch 126/200\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0033 - val_loss: 0.0011 - learning_rate: 1.0000e-06\n\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\nClass weights: {0: 0.4273356401384083, 1: 0.9398782343987824, 2: 1.8269230769230769, 3: 20.583333333333332}\nFalse False False False\nEpoch 1/300\n\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 18ms/step - accuracy: 0.3764 - auc: 0.6278 - loss: 6.0804 - val_accuracy: 0.3531 - val_auc: 0.6677 - val_loss: 3.0419 - learning_rate: 5.0000e-04\nEpoch 2/300\n\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.4674 - auc: 0.7240 - loss: 2.9295 - val_accuracy: 0.5660 - val_auc: 0.8480 - val_loss: 1.3534 - learning_rate: 5.0000e-04\nEpoch 3/300\n\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.5417 - auc: 0.7782 - loss: 3.1912 - val_accuracy: 0.5013 - val_auc: 0.7987 - val_loss: 1.4109 - learning_rate: 5.0000e-04\nEpoch 4/300\n\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.6168 - auc: 0.8628 - loss: 1.2014 - val_accuracy: 0.5768 - val_auc: 0.8403 - val_loss: 1.1924 - learning_rate: 5.0000e-04\nEpoch 5/300\n\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.6616 - auc: 0.8759 - loss: 1.4898 - val_accuracy: 0.6658 - val_auc: 0.8981 - val_loss: 0.8915 - learning_rate: 5.0000e-04\nEpoch 6/300\n\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.7254 - auc: 0.9223 - loss: 1.0612 - val_accuracy: 0.7547 - val_auc: 0.9314 - val_loss: 0.6896 - learning_rate: 5.0000e-04\nEpoch 7/300\n\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.7549 - auc: 0.9342 - loss: 0.7905 - val_accuracy: 0.6819 - val_auc: 0.9154 - val_loss: 0.7937 - learning_rate: 5.0000e-04\nEpoch 8/300\n\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.7542 - auc: 0.9305 - loss: 1.0954 - val_accuracy: 0.8410 - val_auc: 0.9786 - val_loss: 0.3253 - learning_rate: 5.0000e-04\nEpoch 9/300\n\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.7956 - auc: 0.9544 - loss: 0.6536 - val_accuracy: 0.8598 - val_auc: 0.9783 - val_loss: 0.3341 - learning_rate: 5.0000e-04\nEpoch 10/300\n\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.8077 - auc: 0.9555 - loss: 0.5830 - val_accuracy: 0.7493 - val_auc: 0.9398 - val_loss: 0.6456 - learning_rate: 5.0000e-04\nEpoch 11/300\n\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.7744 - auc: 0.9531 - loss: 0.6794 - val_accuracy: 0.8248 - val_auc: 0.9676 - val_loss: 0.4482 - learning_rate: 5.0000e-04\nEpoch 12/300\n\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.8288 - auc: 0.9646 - loss: 0.4191 - val_accuracy: 0.8733 - val_auc: 0.9843 - val_loss: 0.2797 - learning_rate: 5.0000e-04\nEpoch 13/300\n\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8400 - auc: 0.9691 - loss: 0.4823 - val_accuracy: 0.8221 - val_auc: 0.9746 - val_loss: 0.3822 - learning_rate: 5.0000e-04\nEpoch 14/300\n\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8434 - auc: 0.9742 - loss: 0.3836 - val_accuracy: 0.8464 - val_auc: 0.9794 - val_loss: 0.3244 - learning_rate: 5.0000e-04\nEpoch 15/300\n\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8778 - auc: 0.9835 - loss: 0.2360 - val_accuracy: 0.8571 - val_auc: 0.9804 - val_loss: 0.3106 - learning_rate: 5.0000e-04\nEpoch 16/300\n\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8586 - auc: 0.9793 - loss: 0.2659 - val_accuracy: 0.8383 - val_auc: 0.9720 - val_loss: 0.4485 - learning_rate: 5.0000e-04\nEpoch 17/300\n\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8590 - auc: 0.9758 - loss: 0.3536 - val_accuracy: 0.8598 - val_auc: 0.9801 - val_loss: 0.3544 - learning_rate: 5.0000e-04\nEpoch 18/300\n\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8749 - auc: 0.9787 - loss: 0.3243 - val_accuracy: 0.8140 - val_auc: 0.9659 - val_loss: 0.4818 - learning_rate: 5.0000e-04\nEpoch 19/300\n\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8539 - auc: 0.9752 - loss: 0.3710 - val_accuracy: 0.7709 - val_auc: 0.9457 - val_loss: 0.6547 - learning_rate: 5.0000e-04\nEpoch 20/300\n\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8262 - auc: 0.9592 - loss: 0.5063 - val_accuracy: 0.8167 - val_auc: 0.9575 - val_loss: 0.6964 - learning_rate: 5.0000e-04\nEpoch 21/300\n\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8008 - auc: 0.9423 - loss: 1.2745 - val_accuracy: 0.8571 - val_auc: 0.9774 - val_loss: 0.3560 - learning_rate: 5.0000e-04\nEpoch 22/300\n\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.8483 - auc: 0.9739 - loss: 0.5886 - val_accuracy: 0.8868 - val_auc: 0.9875 - val_loss: 0.2438 - learning_rate: 5.0000e-04\nEpoch 23/300\n\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8331 - auc: 0.9640 - loss: 0.5872 - val_accuracy: 0.8410 - val_auc: 0.9711 - val_loss: 0.4828 - learning_rate: 5.0000e-04\nEpoch 24/300\n\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8183 - auc: 0.9598 - loss: 0.7697 - val_accuracy: 0.8895 - val_auc: 0.9868 - val_loss: 0.2508 - learning_rate: 5.0000e-04\nEpoch 25/300\n\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8795 - auc: 0.9815 - loss: 0.3447 - val_accuracy: 0.8625 - val_auc: 0.9783 - val_loss: 0.3496 - learning_rate: 5.0000e-04\nEpoch 26/300\n\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8682 - auc: 0.9779 - loss: 0.4445 - val_accuracy: 0.8679 - val_auc: 0.9834 - val_loss: 0.3058 - learning_rate: 5.0000e-04\nEpoch 27/300\n\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8722 - auc: 0.9733 - loss: 0.5367 - val_accuracy: 0.8571 - val_auc: 0.9748 - val_loss: 0.3893 - learning_rate: 5.0000e-04\nEpoch 28/300\n\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8666 - auc: 0.9796 - loss: 0.3486 - val_accuracy: 0.7547 - val_auc: 0.9567 - val_loss: 0.5588 - learning_rate: 5.0000e-04\nEpoch 29/300\n\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8514 - auc: 0.9747 - loss: 0.4917 - val_accuracy: 0.8949 - val_auc: 0.9856 - val_loss: 0.2688 - learning_rate: 5.0000e-04\nEpoch 30/300\n\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8931 - auc: 0.9834 - loss: 0.2778 - val_accuracy: 0.7278 - val_auc: 0.9357 - val_loss: 0.7827 - learning_rate: 5.0000e-04\nEpoch 31/300\n\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8594 - auc: 0.9696 - loss: 0.5281 - val_accuracy: 0.8544 - val_auc: 0.9830 - val_loss: 0.2849 - learning_rate: 5.0000e-04\nEpoch 32/300\n\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8954 - auc: 0.9849 - loss: 0.3184 - val_accuracy: 0.8760 - val_auc: 0.9831 - val_loss: 0.2902 - learning_rate: 5.0000e-04\nEpoch 33/300\n\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9078 - auc: 0.9901 - loss: 0.1904 - val_accuracy: 0.8949 - val_auc: 0.9912 - val_loss: 0.2074 - learning_rate: 2.5000e-04\nEpoch 34/300\n\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9127 - auc: 0.9909 - loss: 0.2247 - val_accuracy: 0.8895 - val_auc: 0.9892 - val_loss: 0.2360 - learning_rate: 2.5000e-04\nEpoch 35/300\n\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9044 - auc: 0.9908 - loss: 0.1779 - val_accuracy: 0.8005 - val_auc: 0.9613 - val_loss: 0.5425 - learning_rate: 2.5000e-04\nEpoch 36/300\n\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9073 - auc: 0.9905 - loss: 0.1520 - val_accuracy: 0.9407 - val_auc: 0.9954 - val_loss: 0.1491 - learning_rate: 2.5000e-04\nEpoch 37/300\n\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9199 - auc: 0.9914 - loss: 0.1839 - val_accuracy: 0.9218 - val_auc: 0.9937 - val_loss: 0.1780 - learning_rate: 2.5000e-04\nEpoch 38/300\n\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9156 - auc: 0.9923 - loss: 0.1670 - val_accuracy: 0.9245 - val_auc: 0.9946 - val_loss: 0.1601 - learning_rate: 2.5000e-04\nEpoch 39/300\n\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9274 - auc: 0.9927 - loss: 0.1579 - val_accuracy: 0.9353 - val_auc: 0.9926 - val_loss: 0.1938 - learning_rate: 2.5000e-04\nEpoch 40/300\n\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9265 - auc: 0.9931 - loss: 0.1659 - val_accuracy: 0.8787 - val_auc: 0.9886 - val_loss: 0.2440 - learning_rate: 2.5000e-04\nEpoch 41/300\n\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9187 - auc: 0.9918 - loss: 0.1910 - val_accuracy: 0.9191 - val_auc: 0.9920 - val_loss: 0.1835 - learning_rate: 2.5000e-04\nEpoch 42/300\n\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9296 - auc: 0.9920 - loss: 0.1577 - val_accuracy: 0.9245 - val_auc: 0.9946 - val_loss: 0.1628 - learning_rate: 2.5000e-04\nEpoch 43/300\n\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9292 - auc: 0.9936 - loss: 0.1323 - val_accuracy: 0.9299 - val_auc: 0.9945 - val_loss: 0.1649 - learning_rate: 2.5000e-04\nEpoch 44/300\n\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9279 - auc: 0.9942 - loss: 0.1207 - val_accuracy: 0.9272 - val_auc: 0.9938 - val_loss: 0.1770 - learning_rate: 2.5000e-04\nEpoch 45/300\n\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9445 - auc: 0.9944 - loss: 0.1236 - val_accuracy: 0.8410 - val_auc: 0.9809 - val_loss: 0.3503 - learning_rate: 2.5000e-04\nEpoch 46/300\n\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9185 - auc: 0.9923 - loss: 0.1517 - val_accuracy: 0.9407 - val_auc: 0.9961 - val_loss: 0.1383 - learning_rate: 2.5000e-04\nEpoch 47/300\n\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9429 - auc: 0.9951 - loss: 0.1149 - val_accuracy: 0.9353 - val_auc: 0.9957 - val_loss: 0.1446 - learning_rate: 2.5000e-04\nEpoch 48/300\n\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9338 - auc: 0.9940 - loss: 0.1245 - val_accuracy: 0.8976 - val_auc: 0.9869 - val_loss: 0.2597 - learning_rate: 2.5000e-04\nEpoch 49/300\n\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9374 - auc: 0.9933 - loss: 0.1409 - val_accuracy: 0.8329 - val_auc: 0.9749 - val_loss: 0.4162 - learning_rate: 2.5000e-04\nEpoch 50/300\n\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9231 - auc: 0.9921 - loss: 0.1364 - val_accuracy: 0.9111 - val_auc: 0.9929 - val_loss: 0.1905 - learning_rate: 2.5000e-04\nEpoch 51/300\n\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9397 - auc: 0.9944 - loss: 0.1177 - val_accuracy: 0.9326 - val_auc: 0.9958 - val_loss: 0.1476 - learning_rate: 2.5000e-04\nEpoch 52/300\n\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9287 - auc: 0.9940 - loss: 0.1349 - val_accuracy: 0.7925 - val_auc: 0.9634 - val_loss: 0.5374 - learning_rate: 2.5000e-04\nEpoch 53/300\n\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9166 - auc: 0.9896 - loss: 0.1667 - val_accuracy: 0.9245 - val_auc: 0.9947 - val_loss: 0.1629 - learning_rate: 2.5000e-04\nEpoch 54/300\n\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9395 - auc: 0.9947 - loss: 0.1175 - val_accuracy: 0.8976 - val_auc: 0.9903 - val_loss: 0.2121 - learning_rate: 2.5000e-04\nEpoch 55/300\n\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9399 - auc: 0.9946 - loss: 0.1087 - val_accuracy: 0.8464 - val_auc: 0.9829 - val_loss: 0.3436 - learning_rate: 2.5000e-04\nEpoch 56/300\n\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8932 - auc: 0.9827 - loss: 0.3069 - val_accuracy: 0.8679 - val_auc: 0.9822 - val_loss: 0.3327 - learning_rate: 2.5000e-04\nEpoch 57/300\n\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8975 - auc: 0.9895 - loss: 0.2153 - val_accuracy: 0.8922 - val_auc: 0.9907 - val_loss: 0.2181 - learning_rate: 1.2500e-04\nEpoch 58/300\n\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9395 - auc: 0.9965 - loss: 0.1094 - val_accuracy: 0.9272 - val_auc: 0.9928 - val_loss: 0.1943 - learning_rate: 1.2500e-04\nEpoch 59/300\n\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9325 - auc: 0.9939 - loss: 0.1193 - val_accuracy: 0.9380 - val_auc: 0.9962 - val_loss: 0.1381 - learning_rate: 1.2500e-04\nEpoch 60/300\n\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9535 - auc: 0.9973 - loss: 0.0921 - val_accuracy: 0.8895 - val_auc: 0.9906 - val_loss: 0.2252 - learning_rate: 1.2500e-04\nEpoch 61/300\n\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9477 - auc: 0.9943 - loss: 0.1005 - val_accuracy: 0.8949 - val_auc: 0.9898 - val_loss: 0.2398 - learning_rate: 1.2500e-04\nEpoch 62/300\n\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9395 - auc: 0.9965 - loss: 0.0991 - val_accuracy: 0.8976 - val_auc: 0.9899 - val_loss: 0.2348 - learning_rate: 1.2500e-04\nEpoch 63/300\n\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9435 - auc: 0.9957 - loss: 0.0989 - val_accuracy: 0.9299 - val_auc: 0.9946 - val_loss: 0.1658 - learning_rate: 1.2500e-04\nEpoch 64/300\n\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9378 - auc: 0.9953 - loss: 0.1068 - val_accuracy: 0.9164 - val_auc: 0.9926 - val_loss: 0.1753 - learning_rate: 1.2500e-04\nEpoch 65/300\n\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9472 - auc: 0.9967 - loss: 0.0942 - val_accuracy: 0.9111 - val_auc: 0.9922 - val_loss: 0.1998 - learning_rate: 1.2500e-04\nEpoch 66/300\n\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9305 - auc: 0.9928 - loss: 0.1230 - val_accuracy: 0.8949 - val_auc: 0.9901 - val_loss: 0.2375 - learning_rate: 1.2500e-04\nEpoch 67/300\n\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9621 - auc: 0.9968 - loss: 0.0742 - val_accuracy: 0.9245 - val_auc: 0.9951 - val_loss: 0.1536 - learning_rate: 6.2500e-05\nEpoch 68/300\n\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9530 - auc: 0.9966 - loss: 0.0901 - val_accuracy: 0.9488 - val_auc: 0.9969 - val_loss: 0.1243 - learning_rate: 6.2500e-05\nEpoch 69/300\n\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9652 - auc: 0.9978 - loss: 0.0762 - val_accuracy: 0.9326 - val_auc: 0.9955 - val_loss: 0.1494 - learning_rate: 6.2500e-05\nEpoch 70/300\n\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9378 - auc: 0.9956 - loss: 0.1111 - val_accuracy: 0.9353 - val_auc: 0.9953 - val_loss: 0.1534 - learning_rate: 6.2500e-05\nEpoch 71/300\n\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9541 - auc: 0.9975 - loss: 0.0781 - val_accuracy: 0.9084 - val_auc: 0.9933 - val_loss: 0.1841 - learning_rate: 6.2500e-05\nEpoch 72/300\n\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9472 - auc: 0.9967 - loss: 0.0893 - val_accuracy: 0.8976 - val_auc: 0.9917 - val_loss: 0.2129 - learning_rate: 6.2500e-05\nEpoch 73/300\n\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9589 - auc: 0.9977 - loss: 0.0769 - val_accuracy: 0.9299 - val_auc: 0.9954 - val_loss: 0.1578 - learning_rate: 6.2500e-05\nEpoch 74/300\n\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9319 - auc: 0.9950 - loss: 0.1262 - val_accuracy: 0.9191 - val_auc: 0.9941 - val_loss: 0.1728 - learning_rate: 6.2500e-05\nEpoch 75/300\n\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9551 - auc: 0.9974 - loss: 0.0770 - val_accuracy: 0.9380 - val_auc: 0.9964 - val_loss: 0.1326 - learning_rate: 6.2500e-05\nEpoch 76/300\n\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9367 - auc: 0.9951 - loss: 0.1309 - val_accuracy: 0.9245 - val_auc: 0.9949 - val_loss: 0.1578 - learning_rate: 6.2500e-05\nEpoch 77/300\n\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9636 - auc: 0.9974 - loss: 0.0817 - val_accuracy: 0.9084 - val_auc: 0.9921 - val_loss: 0.2088 - learning_rate: 6.2500e-05\nEpoch 78/300\n\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9520 - auc: 0.9971 - loss: 0.0787 - val_accuracy: 0.9488 - val_auc: 0.9964 - val_loss: 0.1328 - learning_rate: 6.2500e-05\nEpoch 79/300\n\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9626 - auc: 0.9973 - loss: 0.0885 - val_accuracy: 0.9191 - val_auc: 0.9940 - val_loss: 0.1757 - learning_rate: 3.1250e-05\nEpoch 80/300\n\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9448 - auc: 0.9968 - loss: 0.0808 - val_accuracy: 0.9137 - val_auc: 0.9919 - val_loss: 0.2100 - learning_rate: 3.1250e-05\nEpoch 81/300\n\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9460 - auc: 0.9968 - loss: 0.0842 - val_accuracy: 0.9461 - val_auc: 0.9966 - val_loss: 0.1296 - learning_rate: 3.1250e-05\nEpoch 82/300\n\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9650 - auc: 0.9983 - loss: 0.0751 - val_accuracy: 0.9380 - val_auc: 0.9963 - val_loss: 0.1347 - learning_rate: 3.1250e-05\nEpoch 83/300\n\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9507 - auc: 0.9966 - loss: 0.0839 - val_accuracy: 0.9488 - val_auc: 0.9967 - val_loss: 0.1276 - learning_rate: 3.1250e-05\nEpoch 84/300\n\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9502 - auc: 0.9972 - loss: 0.0829 - val_accuracy: 0.9272 - val_auc: 0.9946 - val_loss: 0.1643 - learning_rate: 3.1250e-05\nEpoch 85/300\n\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9566 - auc: 0.9978 - loss: 0.0691 - val_accuracy: 0.9245 - val_auc: 0.9944 - val_loss: 0.1678 - learning_rate: 3.1250e-05\nEpoch 86/300\n\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9563 - auc: 0.9980 - loss: 0.0689 - val_accuracy: 0.9353 - val_auc: 0.9959 - val_loss: 0.1429 - learning_rate: 3.1250e-05\nEpoch 87/300\n\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9607 - auc: 0.9981 - loss: 0.0760 - val_accuracy: 0.9488 - val_auc: 0.9959 - val_loss: 0.1426 - learning_rate: 3.1250e-05\nEpoch 88/300\n\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9600 - auc: 0.9976 - loss: 0.0759 - val_accuracy: 0.9461 - val_auc: 0.9961 - val_loss: 0.1385 - learning_rate: 3.1250e-05\nAutoencoder model saved as 'autoencoder_model.h5'\nSupervised model saved as 'supervised_model.h5'\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\nimport matplotlib.pyplot as plt\n  \ndef perform_inference_with_actigraphy(autoencoder, encoder, supervised_model, X_unseen, extra_features_cols, actigraphy_data, impute_strategy='median'):\n    \n    X_unseen_extra, X_labeled_actigraphy, X_unseen_base, actigraphy_flags=preprocess_data(X_unlabeled, X_unseen, extra_features_cols, \n                                                                                              actigraphy_data, is_training=False, normalize=True)\n\n    # Encode the base features\n    X_unseen_encoded = encoder.predict(X_unseen_base) \n\n    # Make predictions using the supervised model\n    y_pred_probs = supervised_model.predict([X_unseen_encoded, X_unseen_extra, X_labeled_actigraphy, actigraphy_flags])\n    #[encoded_features, X_labeled_extra, X_labeled_actigraphy, actigraphy_flags]\n\n    return y_pred_probs\n\n\n#X_test_inference = X_test.drop(columns='id')\n\n# Filter actigraphy data for participants in the training set\ntest_actigraphy_data = {pid: actigraphy_data[pid] for pid in X_test['id'] if pid in actigraphy_data}\n\n# Check the number of participants with actigraphy data in the training set\nprint(f\"Number of participants with actigraphy data in testing set: {len(test_actigraphy_data)}\")\n\ny_pred_probs = perform_inference_with_actigraphy(\n    autoencoder=autoencoder,\n    encoder=encoder,\n    supervised_model=supervised_model,\n    X_unseen=X_test,#,_inference,\n    extra_features_cols=columns,\n    actigraphy_data=test_actigraphy_data\n)\n\nprint(y_pred_probs)\n# Convert predicted probabilities to class labels\nclass_labels = np.argmax(y_pred_probs, axis=1)","metadata":{"execution":{"iopub.status.busy":"2024-11-06T12:07:35.734063Z","iopub.execute_input":"2024-11-06T12:07:35.734539Z","iopub.status.idle":"2024-11-06T12:08:30.084667Z","shell.execute_reply.started":"2024-11-06T12:07:35.734496Z","shell.execute_reply":"2024-11-06T12:08:30.083422Z"},"trusted":true},"execution_count":41,"outputs":[{"name":"stdout","text":"Number of participants with actigraphy data in testing set: 100\n['Basic_Demos-Enroll_Season', 'Basic_Demos-Age', 'Basic_Demos-Sex', 'CGAS-Season', 'CGAS-CGAS_Score', 'Physical-Season', 'Physical-BMI', 'Physical-Height', 'Physical-Weight', 'Physical-Waist_Circumference', 'Physical-Diastolic_BP', 'Physical-HeartRate', 'Physical-Systolic_BP', 'Fitness_Endurance-Season', 'Fitness_Endurance-Max_Stage', 'Fitness_Endurance-Time_Mins', 'Fitness_Endurance-Time_Sec', 'FGC-Season', 'FGC-FGC_CU', 'FGC-FGC_CU_Zone', 'FGC-FGC_GSND', 'FGC-FGC_GSND_Zone', 'FGC-FGC_GSD', 'FGC-FGC_GSD_Zone', 'FGC-FGC_PU', 'FGC-FGC_PU_Zone', 'FGC-FGC_SRL', 'FGC-FGC_SRL_Zone', 'FGC-FGC_SRR', 'FGC-FGC_SRR_Zone', 'FGC-FGC_TL', 'FGC-FGC_TL_Zone', 'BIA-Season', 'BIA-BIA_Activity_Level_num', 'BIA-BIA_BMC', 'BIA-BIA_BMI', 'BIA-BIA_BMR', 'BIA-BIA_DEE', 'BIA-BIA_ECW', 'BIA-BIA_FFM', 'BIA-BIA_FFMI', 'BIA-BIA_FMI', 'BIA-BIA_Fat', 'BIA-BIA_Frame_num', 'BIA-BIA_ICW', 'BIA-BIA_LDM', 'BIA-BIA_LST', 'BIA-BIA_SMM', 'BIA-BIA_TBW', 'PAQ_A-Season', 'PAQ_A-PAQ_A_Total', 'PAQ_C-Season', 'PAQ_C-PAQ_C_Total', 'SDS-Season', 'SDS-SDS_Total_Raw', 'SDS-SDS_Total_T', 'PreInt_EduHx-Season', 'PreInt_EduHx-computerinternet_hoursday']\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/impute/_iterative.py:785: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"x labeled actigraphy (396, 6)\n\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step\n[[9.9971545e-01 2.8457685e-04 3.6220037e-14 9.8953942e-31]\n [9.9999994e-01 2.0073338e-12 1.7052541e-30 0.0000000e+00]\n [9.9999863e-01 1.3497438e-06 2.2832243e-19 0.0000000e+00]\n ...\n [9.9487358e-01 5.1263645e-03 4.7883568e-12 6.4424143e-28]\n [5.6904787e-03 9.9424839e-01 6.1151361e-05 6.1793158e-17]\n [9.9976677e-01 2.3327839e-04 5.3629146e-15 1.2523045e-32]]\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix, f1_score\n\n# Print predicted class labels\nprint(\"Predicted class labels:\", class_labels)\n\n# Now, to print predicted vs actual class labels\n# Ensure y_test is also in the same format as class_labels\nif len(y_test) == len(class_labels):\n    results_df = pd.DataFrame({\n        'Actual': y_test.values,        # Convert to numpy array for compatibility\n        'Predicted': class_labels\n    })\n    print(results_df)\nelse:\n    print(\"Mismatch in length between actual and predicted labels.\")\n    \n# Exclude NaN values from y_test and class_labels\nvalid_indices = ~np.isnan(y_test) & ~np.isnan(class_labels)\ny_test_clean = y_test[valid_indices]\nclass_labels_clean = class_labels[valid_indices]\n\n# Check if the lengths match\nif len(y_test_clean) == len(class_labels_clean):\n    # Calculate the F1 score\n    f1 = f1_score(y_test_clean, class_labels_clean, average='weighted')  # Use 'weighted' to handle class imbalance\n    print(f\"F1 Score (Weighted): {f1:.4f}\")\n    \n    # Calculate the confusion matrix with non-NaN values\n    cm = confusion_matrix(y_test_clean, class_labels_clean)\n\n    # Create a DataFrame for better visualization\n    cm_df = pd.DataFrame(cm, \n                         index=[f'Class {i}' for i in range(cm.shape[0])], \n                         columns=[f'Class {i}' for i in range(cm.shape[1])])\n\n    # Plot the confusion matrix\n    plt.figure(figsize=(10, 7))\n    sns.heatmap(cm_df, annot=True, fmt='d', cmap='Blues', cbar=False)\n    plt.title('Confusion Matrix (Excluding NaN Values)')\n    plt.ylabel('Actual')\n    plt.xlabel('Predicted')\n    plt.show()\nelse:\n    print(\"Mismatch in length between actual and predicted labels after filtering NaNs.\")\n","metadata":{"execution":{"iopub.status.busy":"2024-11-06T12:08:41.566309Z","iopub.execute_input":"2024-11-06T12:08:41.567243Z","iopub.status.idle":"2024-11-06T12:08:41.805552Z","shell.execute_reply.started":"2024-11-06T12:08:41.567193Z","shell.execute_reply":"2024-11-06T12:08:41.803979Z"},"trusted":true},"execution_count":42,"outputs":[{"name":"stdout","text":"Predicted class labels: [0 0 0 0 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 2 0 2 0 1 0 2 0 0 0\n 0 0 0 1 0 1 0 3 2 0 0 0 0 1 0 2 0 0 0 0 0 0 1 0 0 0 0 1 1 3 0 0 2 1 2 1 0\n 1 0 2 0 0 0 0 0 0 0 0 1 0 0 0 0 2 0 0 0 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0 1 0\n 0 1 0 0 0 0 0 0 2 2 0 0 2 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0\n 0 0 2 2 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 2 0 0 0 0 0 1 2 0 0 0 0 0 0 1\n 0 0 0 0 0 0 0 0 0 0 0 1 0 2 0 1 0 0 0 0 1 1 0 2 0 0 0 0 0 1 0 0 0 0 0 1 1\n 0 0 0 0 2 0 0 0 0 0 0 0 0 0 2 1 0 2 0 1 0 0 0 0 1 3 1 0 0 0 0 0 0 0 0 0 3\n 0 0 0 1 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 1 0 0 2 0 0 1 0 0 1\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 1 2 0 0 0 2 0 0 1 0 0 0 0 0 0 0 0 3 1 0 0 0 0 0 0 2 0 0 1\n 0 0 1 0 0 1 2 0 0 0 0 0 2 0 0 0 0 2 0 1 0 0 0 0 1 0]\n     Actual  Predicted\n0       NaN          0\n1       0.0          0\n2       0.0          0\n3       NaN          0\n4       1.0          1\n..      ...        ...\n391     1.0          0\n392     NaN          0\n393     NaN          0\n394     1.0          1\n395     0.0          0\n\n[396 rows x 2 columns]\nF1 Score (Weighted): 0.8452\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 1000x700 with 1 Axes>","image/png":"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"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}